#!/usr/bin/env python3
"""
å¯¹æ¯” KORGym æ¸¸æˆè¯„ä¼°ç»“æœ
æ”¯æŒå¤šç§æ¸¸æˆç±»å‹ï¼Œè‡ªåŠ¨è¯†åˆ«æ¸¸æˆå¹¶ä½¿ç”¨å¯¹åº”çš„è®ºæ–‡åŸºå‡†

Usage:
    python scripts/compare_korgym_scores.py \
        workspace/korgym_eval/baseline.json \
        workspace/korgym_eval/enhanced.json
"""

import argparse
import json
from pathlib import Path
from typing import List, Dict, Optional


# å„æ¸¸æˆçš„è®ºæ–‡åŸºå‡†åˆ†æ•°ï¼ˆæ¥è‡ª KORGym è®ºæ–‡ï¼‰
PAPER_SCORES = {
    'Word Problem': {
        'O1-2024-12-17': 0.960,
        'Gemini-2.5-pro-03-25': 0.900,
        'Claude-3.7-thinking': 0.820,
        'DeepSeek-R1': 0.820,
        'O3-mini': 0.880,
        'Gemini-2.0-Flash-thinking': 0.620,
        'Claude-3.7': 0.580,
        'DeepSeek-v3-0324': 0.460,
        'GPT-4o': 0.420,
        'Doubao-1.5-thinking-pro': 0.600,
        'Gemini-2.0-Flash': 0.340,
        'Doubao-1.5-pro': 0.120,
        'DeepSeek-R1-Distill-Qwen-32B': 0.340,
        'Qwen-Max': 0.480,
        'DeepSeek-R1-Distill-Qwen-7B': 0.020,
    },
    'Alphabetical Sorting': {
        # å¦‚æœæœ‰è®ºæ–‡æ•°æ®ï¼Œåœ¨è¿™é‡Œæ·»åŠ 
        # ç›®å‰ä¸ºç©ºï¼Œåªå¯¹æ¯”è‡ªå·±çš„ç»“æœ
    },
    # å¯ä»¥ç»§ç»­æ·»åŠ å…¶ä»–æ¸¸æˆçš„è®ºæ–‡åˆ†æ•°
}

# æ¸¸æˆåç§°æ˜ å°„ï¼ˆå¤„ç†ä¸åŒçš„å‘½åæ–¹å¼ï¼‰
GAME_NAME_MAPPING = {
    '8-word_puzzle': 'Word Problem',
    'word_puzzle': 'Word Problem',
    'Word Problem': 'Word Problem',
    '22-alphabetical_sorting': 'Alphabetical Sorting',
    'alphabetical_sorting': 'Alphabetical Sorting',
    'Alphabetical Sorting': 'Alphabetical Sorting',
}


def detect_game_type(result: Dict) -> Optional[str]:
    """ä»è¯„ä¼°ç»“æœä¸­æ£€æµ‹æ¸¸æˆç±»å‹"""
    # å°è¯•ä»ä¸åŒå­—æ®µæ£€æµ‹
    game_hints = [
        result.get('game_name', ''),
        result.get('dataset_name', ''),
        result.get('exp_id', ''),
        str(result.get('detailed_results', [{}])[0].get('seed', '') if result.get('detailed_results') else ''),
    ]
    
    for hint in game_hints:
        hint_lower = hint.lower()
        if 'word_puzzle' in hint_lower or 'word problem' in hint_lower:
            return 'Word Problem'
        elif 'alphabetical' in hint_lower or '22-alphabetical' in hint_lower:
            return 'Alphabetical Sorting'
    
    return None


def load_result(json_path: str) -> Dict:
    """åŠ è½½è¯„ä¼°ç»“æœ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # è§„èŒƒåŒ–å­—æ®µå
    if 'average_score' in data and 'avg_score' not in data:
        data['avg_score'] = data['average_score']
    elif 'avg_score' in data and 'average_score' not in data:
        data['average_score'] = data['avg_score']
    
    return data


def print_comparison(results: List[Dict], game_type: str):
    """æ‰“å°å¯¹æ¯”ç»“æœ"""
    
    print("\n" + "=" * 80)
    print(f"  {game_type} - Score Comparison")
    print("=" * 80)
    print()
    
    # æ‰“å°ä½ çš„ç»“æœ
    print("ğŸ“Š Your Results:")
    print(f"{'Experiment':<50} {'Score':>10} {'Games':>10}")
    print("-" * 80)
    
    for result in results:
        score = result.get('avg_score') or result.get('average_score', 0)
        exp_name = Path(result.get('exp_id', 'Unknown')).stem
        num_games = result.get('num_games', 'N/A')
        
        print(f"{exp_name:<50} {score:>10.4f} {num_games:>10}")
    
    print()
    
    # å¦‚æœæœ‰å¤šä¸ªç»“æœï¼Œæ˜¾ç¤ºæå‡
    if len(results) > 1:
        baseline = results[0].get('avg_score') or results[0].get('average_score', 0)
        enhanced = results[-1].get('avg_score') or results[-1].get('average_score', 0)
        improvement = enhanced - baseline
        improvement_pct = (improvement / baseline * 100) if baseline > 0 else 0
        
        print("ğŸ“ˆ Improvement:")
        print(f"  Baseline:    {baseline:.4f}")
        print(f"  Enhanced:    {enhanced:.4f}")
        print(f"  Improvement: {improvement:+.4f} ({improvement_pct:+.1f}%)")
        print()
    
    # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    print("ğŸ“‰ Detailed Statistics:")
    print(f"{'Experiment':<50} {'Mean':>8} {'Std':>8} {'Min':>8} {'Max':>8} {'Success%':>10}")
    print("-" * 80)
    for result in results:
        exp_name = Path(result.get('exp_id', 'Unknown')).stem
        mean = result.get('avg_score') or result.get('average_score', 0)
        std = result.get('std_score', 0)
        min_score = result.get('min_score', 0)
        max_score = result.get('max_score', 0)
        success_rate = result.get('success_rate', 0) * 100
        
        print(f"{exp_name:<50} {mean:>8.4f} {std:>8.4f} {min_score:>8.4f} {max_score:>8.4f} {success_rate:>9.1f}%")
    print()
    
    # å¦‚æœæœ‰è®ºæ–‡åŸºå‡†ï¼Œæ˜¾ç¤ºæ’å
    paper_scores = PAPER_SCORES.get(game_type, {})
    if paper_scores:
        print(f"ğŸ“– Paper Benchmark Comparison ({game_type}):")
        print(f"{'Model':<50} {'Score':>10}")
        print("-" * 80)
        
        # åˆå¹¶ä½ çš„ç»“æœå’Œè®ºæ–‡ç»“æœ
        all_scores = []
        for result in results:
            score = result.get('avg_score') or result.get('average_score', 0)
            exp_name = "â†’ " + Path(result.get('exp_id', 'Your Model')).stem
            all_scores.append((exp_name, score, True))
        
        for model, score in paper_scores.items():
            all_scores.append((model, score, False))
        
        # æŒ‰åˆ†æ•°é™åºæ’åˆ—
        all_scores.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰“å°æ’å
        for i, (model, score, is_yours) in enumerate(all_scores, 1):
            if is_yours:
                print(f"{i:2d}. {model:<48} {score:>10.4f} â­")
            else:
                print(f"{i:2d}. {model:<48} {score:>10.4f}")
        
        print()
        
        # ç»Ÿè®¡ä½ çš„æ’å
        for result in results:
            score = result.get('avg_score') or result.get('average_score', 0)
            exp_name = Path(result.get('exp_id', 'Unknown')).stem
            rank = sum(1 for _, s, _ in all_scores if s > score) + 1
            total = len(all_scores)
            better_than = sum(1 for _, s, is_yours in all_scores 
                            if not is_yours and s < score)
            total_paper = sum(1 for _, _, is_yours in all_scores if not is_yours)
            
            print(f"  {exp_name}: Rank {rank}/{total} (better than {better_than}/{total_paper} paper models)")
        
        print("=" * 80)
    else:
        print(f"â„¹ï¸  No paper benchmark data available for {game_type}")
        print(f"   Only showing comparison between your experiments")
        print("=" * 80)
    
    print()


def main():
    parser = argparse.ArgumentParser(
        description="Compare KORGym evaluation results with optional paper benchmarks"
    )
    parser.add_argument(
        'results',
        nargs='+',
        help='Paths to result JSON files (e.g., baseline.json enhanced.json)'
    )
    parser.add_argument(
        '--game-type',
        type=str,
        default=None,
        help='Manually specify game type (e.g., "Word Problem", "Alphabetical Sorting")'
    )
    
    args = parser.parse_args()
    
    # åŠ è½½æ‰€æœ‰ç»“æœ
    results = []
    for path in args.results:
        try:
            result = load_result(path)
            results.append(result)
        except Exception as e:
            print(f"âŒ Error loading {path}: {e}")
            continue
    
    if not results:
        print("âŒ No valid results loaded")
        return
    
    # æ£€æµ‹æ¸¸æˆç±»å‹
    if args.game_type:
        game_type = args.game_type
    else:
        game_type = detect_game_type(results[0])
        if not game_type:
            game_type = "Unknown Game"
            print(f"âš ï¸  Could not auto-detect game type. Use --game-type to specify.")
    
    print(f"\nğŸ® Detected Game Type: {game_type}")
    
    # æ‰“å°å¯¹æ¯”
    print_comparison(results, game_type)


if __name__ == "__main__":
    main()








