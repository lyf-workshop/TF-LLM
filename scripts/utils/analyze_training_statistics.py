#!/usr/bin/env python3
"""
åˆ†æè®­ç»ƒå‰åçš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯

ç”¨æ³•:
    python scripts/analyze_training_statistics.py --baseline_exp_id <baseline_id> --practice_exp_id <practice_id>
"""

import sys
from pathlib import Path
from sqlmodel import select, func
from collections import defaultdict

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utu.db.eval_datapoint import EvaluationSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def calculate_pass_at_k(samples, k):
    """è®¡ç®— Pass@K"""
    problem_to_samples = defaultdict(list)
    for sample in samples:
        key = sample.raw_question or sample.question
        problem_to_samples[key].append(sample)
    
    solved_problems = 0
    for problem, problem_samples in problem_to_samples.items():
        # å–å‰kä¸ªæ ·æœ¬
        samples_k = problem_samples[:k]
        # å¦‚æœä»»æ„ä¸€ä¸ªæ­£ç¡®ï¼Œåˆ™é—®é¢˜è¢«è§£å†³
        if any(s.reward and s.reward > 0.5 for s in samples_k):
            solved_problems += 1
    
    return solved_problems / len(problem_to_samples) if problem_to_samples else 0.0


def analyze_statistics(baseline_exp_id: str, practice_exp_id: str):
    """åˆ†æè®­ç»ƒå‰åçš„è¯¦ç»†ç»Ÿè®¡"""
    
    print("\n" + "=" * 80)
    print("ğŸ“Š è®­ç»ƒå‰åè¯¦ç»†ç»Ÿè®¡åˆ†æ")
    print("=" * 80)
    print(f"\nBaseline å®éªŒ: {baseline_exp_id}")
    print(f"Practice å®éªŒ: {practice_exp_id}")
    print()
    
    with SQLModelUtils.create_session() as session:
        # è·å– baseline ç»“æœ
        print("ğŸ“ˆ Baseline ç»Ÿè®¡:")
        print("-" * 80)
        baseline_samples = list(session.exec(
            select(EvaluationSample).where(
                EvaluationSample.exp_id == baseline_exp_id
            )
        ))
        
        if not baseline_samples:
            print(f"âŒ æœªæ‰¾åˆ° Baseline æ•°æ®")
            return
        
        # Baseline ç»Ÿè®¡
        baseline_total = len(baseline_samples)
        baseline_correct = sum(1 for s in baseline_samples if s.reward and s.reward > 0.5)
        baseline_accuracy = baseline_correct / baseline_total if baseline_total > 0 else 0.0
        
        # æŒ‰é—®é¢˜åˆ†ç»„
        baseline_problems = defaultdict(list)
        for sample in baseline_samples:
            key = sample.raw_question or sample.question
            baseline_problems[key].append(sample)
        
        baseline_total_problems = len(baseline_problems)
        baseline_solved_problems = sum(
            1 for problem_samples in baseline_problems.values()
            if any(s.reward and s.reward > 0.5 for s in problem_samples)
        )
        
        # Pass@K
        baseline_pass_at_32 = calculate_pass_at_k(baseline_samples, 32)
        
        print(f"  æ€»æ ·æœ¬æ•°: {baseline_total}")
        print(f"  æ­£ç¡®æ ·æœ¬æ•°: {baseline_correct}")
        print(f"  å‡†ç¡®ç‡ (æ ·æœ¬çº§åˆ«): {baseline_accuracy:.2%} ({baseline_correct}/{baseline_total})")
        print(f"  æ€»é—®é¢˜æ•°: {baseline_total_problems}")
        print(f"  å·²è§£å†³é—®é¢˜æ•°: {baseline_solved_problems}")
        print(f"  é—®é¢˜è§£å†³ç‡: {baseline_solved_problems/baseline_total_problems:.2%} ({baseline_solved_problems}/{baseline_total_problems})")
        print(f"  Pass@32: {baseline_pass_at_32:.2%}")
        
        # è·å– practice ç»“æœ
        print("\nğŸ“ˆ Practice ç»Ÿè®¡:")
        print("-" * 80)
        practice_samples = list(session.exec(
            select(EvaluationSample).where(
                EvaluationSample.exp_id == practice_exp_id
            )
        ))
        
        if not practice_samples:
            print(f"âŒ æœªæ‰¾åˆ° Practice æ•°æ®")
            return
        
        # Practice ç»Ÿè®¡
        practice_total = len(practice_samples)
        practice_correct = sum(1 for s in practice_samples if s.reward and s.reward > 0.5)
        practice_accuracy = practice_correct / practice_total if practice_total > 0 else 0.0
        
        # æŒ‰é—®é¢˜åˆ†ç»„
        practice_problems = defaultdict(list)
        for sample in practice_samples:
            key = sample.raw_question or sample.question
            practice_problems[key].append(sample)
        
        practice_total_problems = len(practice_problems)
        practice_solved_problems = sum(
            1 for problem_samples in practice_problems.values()
            if any(s.reward and s.reward > 0.5 for s in problem_samples)
        )
        
        # Pass@K
        practice_pass_at_32 = calculate_pass_at_k(practice_samples, 32)
        
        print(f"  æ€»æ ·æœ¬æ•°: {practice_total}")
        print(f"  æ­£ç¡®æ ·æœ¬æ•°: {practice_correct}")
        print(f"  å‡†ç¡®ç‡ (æ ·æœ¬çº§åˆ«): {practice_accuracy:.2%} ({practice_correct}/{practice_total})")
        print(f"  æ€»é—®é¢˜æ•°: {practice_total_problems}")
        print(f"  å·²è§£å†³é—®é¢˜æ•°: {practice_solved_problems}")
        print(f"  é—®é¢˜è§£å†³ç‡: {practice_solved_problems/practice_total_problems:.2%} ({practice_solved_problems}/{practice_total_problems})")
        print(f"  Pass@32: {practice_pass_at_32:.2%}")
        
        # å¯¹æ¯”åˆ†æ
        print("\nğŸ“Š å¯¹æ¯”åˆ†æ:")
        print("-" * 80)
        
        # æ ·æœ¬çº§åˆ«æ”¹è¿›
        accuracy_improvement = practice_accuracy - baseline_accuracy
        print(f"  æ ·æœ¬å‡†ç¡®ç‡å˜åŒ–: {baseline_accuracy:.2%} â†’ {practice_accuracy:.2%} ({accuracy_improvement:+.2%})")
        print(f"  æ­£ç¡®æ ·æœ¬æ•°å˜åŒ–: {baseline_correct} â†’ {practice_correct} ({practice_correct - baseline_correct:+d})")
        
        # é—®é¢˜çº§åˆ«æ”¹è¿›
        problem_improvement = practice_solved_problems - baseline_solved_problems
        problem_improvement_rate = (practice_solved_problems / practice_total_problems) - (baseline_solved_problems / baseline_total_problems)
        print(f"  é—®é¢˜è§£å†³ç‡å˜åŒ–: {baseline_solved_problems/baseline_total_problems:.2%} â†’ {practice_solved_problems/practice_total_problems:.2%} ({problem_improvement_rate:+.2%})")
        print(f"  å·²è§£å†³é—®é¢˜æ•°å˜åŒ–: {baseline_solved_problems} â†’ {practice_solved_problems} ({problem_improvement:+d})")
        
        # Pass@32 æ”¹è¿›
        pass_improvement = practice_pass_at_32 - baseline_pass_at_32
        print(f"  Pass@32 å˜åŒ–: {baseline_pass_at_32:.2%} â†’ {practice_pass_at_32:.2%} ({pass_improvement:+.2%})")
        
        # è¯¦ç»†å˜åŒ–åˆ†æ
        print("\nğŸ” è¯¦ç»†å˜åŒ–åˆ†æ:")
        print("-" * 80)
        
        # æ‰¾å‡ºå…±åŒé—®é¢˜
        baseline_question_set = set(baseline_problems.keys())
        practice_question_set = set(practice_problems.keys())
        common_questions = baseline_question_set & practice_question_set
        
        improved = 0
        regressed = 0
        unchanged_correct = 0
        unchanged_incorrect = 0
        
        for question in common_questions:
            baseline_best = max((s.reward for s in baseline_problems[question] if s.reward is not None), default=0.0)
            practice_best = max((s.reward for s in practice_problems[question] if s.reward is not None), default=0.0)
            
            baseline_correct_q = baseline_best > 0.5
            practice_correct_q = practice_best > 0.5
            
            if not baseline_correct_q and practice_correct_q:
                improved += 1
            elif baseline_correct_q and not practice_correct_q:
                regressed += 1
            elif baseline_correct_q and practice_correct_q:
                unchanged_correct += 1
            else:
                unchanged_incorrect += 1
        
        print(f"  å…±åŒé—®é¢˜æ•°: {len(common_questions)}")
        print(f"  âœ… æ”¹è¿›: {improved} ä¸ªï¼ˆé”™è¯¯ â†’ æ­£ç¡®ï¼‰")
        print(f"  âŒ é€€åŒ–: {regressed} ä¸ªï¼ˆæ­£ç¡® â†’ é”™è¯¯ï¼‰")
        print(f"  â¡ï¸  ä¿æŒæ­£ç¡®: {unchanged_correct} ä¸ª")
        print(f"  â¡ï¸  ä¿æŒé”™è¯¯: {unchanged_incorrect} ä¸ª")
        
        # æ€»ç»“
        print("\n" + "=" * 80)
        print("ğŸ“ æ€»ç»“")
        print("=" * 80)
        
        if accuracy_improvement > 0:
            print(f"âœ… è®­ç»ƒæœ‰æ•ˆï¼šæ ·æœ¬å‡†ç¡®ç‡æå‡äº† {accuracy_improvement:.2%}")
        elif accuracy_improvement < 0:
            print(f"âš ï¸  è®­ç»ƒå¯èƒ½æœ‰é—®é¢˜ï¼šæ ·æœ¬å‡†ç¡®ç‡ä¸‹é™äº† {abs(accuracy_improvement):.2%}")
        else:
            print("â¡ï¸  æ ·æœ¬å‡†ç¡®ç‡æ²¡æœ‰å˜åŒ–")
        
        if problem_improvement > 0:
            print(f"âœ… é—®é¢˜è§£å†³ç‡æå‡äº† {problem_improvement_rate:.2%}ï¼Œå¤šè§£å†³äº† {problem_improvement} ä¸ªé—®é¢˜")
        elif problem_improvement < 0:
            print(f"âš ï¸  é—®é¢˜è§£å†³ç‡ä¸‹é™äº† {abs(problem_improvement_rate):.2%}ï¼Œå°‘è§£å†³äº† {abs(problem_improvement)} ä¸ªé—®é¢˜")
        else:
            print("â¡ï¸  é—®é¢˜è§£å†³ç‡æ²¡æœ‰å˜åŒ–")
        
        if improved > regressed:
            print(f"âœ… å‡€æ”¹è¿› {improved - regressed} ä¸ªé—®é¢˜")
        elif improved < regressed:
            print(f"âš ï¸  å‡€é€€åŒ– {regressed - improved} ä¸ªé—®é¢˜")
        else:
            print("â¡ï¸  æ”¹è¿›å’Œé€€åŒ–æ•°é‡ç›¸åŒ")
        
        print()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(
        description="åˆ†æè®­ç»ƒå‰åçš„è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"
    )
    parser.add_argument(
        "--baseline_exp_id",
        type=str,
        default="logic_zebralogic_test_eval",
        help="Baseline å®éªŒ ID"
    )
    parser.add_argument(
        "--practice_exp_id",
        type=str,
        default="logic_practice_zebralogic_test_eval",
        help="Practice å®éªŒ ID"
    )
    
    args = parser.parse_args()
    
    analyze_statistics(args.baseline_exp_id, args.practice_exp_id)


if __name__ == "__main__":
    main()

