#!/usr/bin/env python3
"""æŸ¥çœ‹è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯ï¼ŒåŒ…æ‹¬é—®é¢˜ã€æ¨¡å‹ç­”æ¡ˆã€æ ‡å‡†ç­”æ¡ˆ"""

import argparse
import json
from typing import Optional
from sqlmodel import select

from utu.db.eval_datapoint import EvaluationSample, DatasetSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def view_evaluation_details(exp_id: str, limit: Optional[int] = None, show_correct_only: bool = False, show_wrong_only: bool = False):
    """æŸ¥çœ‹è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯"""
    
    print("\n" + "="*80)
    print(f"å®éªŒè¯¦æƒ…: {exp_id}")
    print("="*80 + "\n")
    
    with SQLModelUtils.create_session() as session:
        # è·å–è¯„ä¼°æ ·æœ¬
        statement = select(EvaluationSample).where(EvaluationSample.exp_id == exp_id)
        samples = list(session.exec(statement))
        
        if not samples:
            print(f"âŒ æœªæ‰¾åˆ°å®éªŒ '{exp_id}' çš„æ•°æ®")
            return
        
        # æŒ‰é—®é¢˜åˆ†ç»„
        problem_to_samples = {}
        for sample in samples:
            key = sample.raw_question or sample.question
            if key not in problem_to_samples:
                problem_to_samples[key] = []
            problem_to_samples[key].append(sample)
        
        print(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(samples)}")
        print(f"  é—®é¢˜æ•°: {len(problem_to_samples)}")
        print(f"  æ¯é¢˜é‡‡æ ·æ•°: {len(samples) // len(problem_to_samples) if problem_to_samples else 0}")
        
        # è®¡ç®—å‡†ç¡®ç‡
        correct_samples = [s for s in samples if s.reward and s.reward > 0.5]
        accuracy = len(correct_samples) / len(samples) if samples else 0
        print(f"  å‡†ç¡®ç‡: {accuracy:.2%} ({len(correct_samples)}/{len(samples)})")
        
        print("\n" + "-"*80 + "\n")
        
        # æ˜¾ç¤ºæ¯ä¸ªé—®é¢˜çš„è¯¦æƒ…
        problem_count = 0
        for problem_idx, (question, problem_samples) in enumerate(problem_to_samples.items(), 1):
            if limit and problem_count >= limit:
                break
            
            # ç»Ÿè®¡è¿™ä¸ªé—®é¢˜çš„æ­£ç¡®ç‡
            correct_count = sum(1 for s in problem_samples if s.reward and s.reward > 0.5)
            problem_accuracy = correct_count / len(problem_samples) if problem_samples else 0
            
            # è¿‡æ»¤æ¡ä»¶
            if show_correct_only and correct_count == 0:
                continue
            if show_wrong_only and correct_count > 0:
                continue
            
            problem_count += 1
            
            print(f"{'='*80}")
            print(f"é—®é¢˜ #{problem_idx} - æ­£ç¡®ç‡: {problem_accuracy:.2%} ({correct_count}/{len(problem_samples)})")
            print(f"{'='*80}\n")
            
            # æ˜¾ç¤ºé—®é¢˜å†…å®¹ï¼ˆæˆªæ–­ï¼‰
            question_preview = question[:200] + "..." if len(question) > 200 else question
            print(f"ğŸ“ é—®é¢˜å†…å®¹:")
            print(f"{question_preview}\n")
            
            # è·å–æ ‡å‡†ç­”æ¡ˆï¼ˆä»æ•°æ®é›†ï¼‰
            first_sample = problem_samples[0]
            if first_sample.data_id:
                dataset_sample = session.get(DatasetSample, first_sample.data_id)
                if dataset_sample and dataset_sample.answer:
                    print(f"âœ… æ ‡å‡†ç­”æ¡ˆ:")
                    try:
                        # å°è¯•è§£æ JSON
                        answer_data = json.loads(dataset_sample.answer)
                        print(json.dumps(answer_data, indent=2, ensure_ascii=False))
                    except json.JSONDecodeError:
                        print(f"{dataset_sample.answer}")
                    print()
            
            # æ˜¾ç¤ºæ¨¡å‹è¾“å‡ºï¼ˆåªæ˜¾ç¤ºå‰å‡ ä¸ªï¼‰
            print(f"ğŸ¤– æ¨¡å‹è¾“å‡ºæ ·æœ¬ (æ˜¾ç¤ºå‰5ä¸ª):\n")
            for i, sample in enumerate(problem_samples[:5], 1):
                reward_symbol = "âœ…" if sample.reward and sample.reward > 0.5 else "âŒ"
                reward_value = sample.reward if sample.reward is not None else 0.0
                
                print(f"  æ ·æœ¬ #{i} {reward_symbol} (reward: {reward_value:.2f})")
                
                # æ˜¾ç¤ºè¾“å‡ºï¼ˆæˆªæ–­ï¼‰
                if sample.output:
                    output_preview = sample.output[:300] + "..." if len(sample.output) > 300 else sample.output
                    print(f"  è¾“å‡º: {output_preview}")
                else:
                    print(f"  è¾“å‡º: (ç©º)")
                
                # æ˜¾ç¤ºæå–çš„ç­”æ¡ˆ
                if hasattr(sample, 'extracted_answer') and sample.extracted_answer:
                    print(f"  æå–çš„ç­”æ¡ˆ: {sample.extracted_answer}")
                
                print()
            
            if len(problem_samples) > 5:
                print(f"  ... è¿˜æœ‰ {len(problem_samples) - 5} ä¸ªæ ·æœ¬\n")
            
            print("-"*80 + "\n")
        
        if limit and problem_count >= limit:
            print(f"\nå·²æ˜¾ç¤º {limit} ä¸ªé—®é¢˜ã€‚ä½¿ç”¨ --limit å‚æ•°æŸ¥çœ‹æ›´å¤šã€‚")


def main():
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹è¯„ä¼°çš„è¯¦ç»†ä¿¡æ¯")
    parser.add_argument(
        "exp_id",
        nargs="?",
        default="logic_zebralogic_test_eval",
        help="å®éªŒID (é»˜è®¤: logic_zebralogic_test_eval)"
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="é™åˆ¶æ˜¾ç¤ºçš„é—®é¢˜æ•°é‡"
    )
    parser.add_argument(
        "--correct",
        action="store_true",
        help="åªæ˜¾ç¤ºè‡³å°‘æœ‰ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„é—®é¢˜"
    )
    parser.add_argument(
        "--wrong",
        action="store_true",
        help="åªæ˜¾ç¤ºå…¨éƒ¨é”™è¯¯çš„é—®é¢˜"
    )
    
    args = parser.parse_args()
    
    view_evaluation_details(
        args.exp_id,
        limit=args.limit,
        show_correct_only=args.correct,
        show_wrong_only=args.wrong
    )


if __name__ == "__main__":
    main()

