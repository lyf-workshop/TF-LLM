#!/usr/bin/env python3
"""
Word Puzzle è®ºæ–‡å¯¹é½è¯„ä¼°è„šæœ¬

ä¸¥æ ¼æŒ‰ç…§ KORGym è®ºæ–‡çš„è¯„ä¼°æ–¹å¼ï¼š
- è¯„ä¼° 50 ä¸ªä¸åŒ seed çš„æ¸¸æˆ
- è®¡ç®—å¹³å‡å¾—åˆ†ï¼ˆä¸è®ºæ–‡è¡¨æ ¼ä¸­çš„æ•°å€¼ä¸€è‡´ï¼‰
- è¾“å‡ºæ ¼å¼ä¸è®ºæ–‡ç›¸åŒ

Usage:
    python scripts/eval_word_puzzle_paper_aligned.py --exp_id my_baseline_eval
"""

import argparse
import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict

import numpy as np
from tqdm import tqdm

from utu.agents import get_agent
from utu.config import ConfigLoader
from utu.practice.korgym_adapter import KORGymAdapter
from utu.utils import get_logger

logger = get_logger(__name__)


class PaperAlignedEvaluator:
    """ä¸ KORGym è®ºæ–‡å¯¹é½çš„è¯„ä¼°å™¨"""
    
    def __init__(
        self,
        agent_config_name: str,
        game_name: str = "8-word_puzzle",
        game_port: int = 8775,
        num_seeds: int = 50,  # è®ºæ–‡ä½¿ç”¨ 50 å±€
        level: int = 4,  # è®ºæ–‡é»˜è®¤ level 4
    ):
        """
        åˆå§‹åŒ–è¯„ä¼°å™¨
        
        Args:
            agent_config_name: Agent é…ç½®åç§°
            game_name: æ¸¸æˆåç§°
            game_port: æ¸¸æˆæœåŠ¡å™¨ç«¯å£
            num_seeds: è¯„ä¼°çš„æ¸¸æˆå±€æ•°ï¼ˆè®ºæ–‡ä½¿ç”¨ 50ï¼‰
            level: æ¸¸æˆéš¾åº¦çº§åˆ«
        """
        self.agent_config_name = agent_config_name
        self.game_name = game_name
        self.num_seeds = num_seeds
        self.level = level
        
        # åˆå§‹åŒ– KORGym Adapter
        self.adapter = KORGymAdapter(
            game_name=game_name,
            game_host="localhost",
            game_port=game_port,
            level=level,
            max_rounds=50
        )
        
        # åŠ è½½ Agent
        logger.info(f"Loading agent config: {agent_config_name}")
        agent_config = ConfigLoader.load_agent_config(agent_config_name)
        self.agent = get_agent(agent_config)
        logger.info(f"âœ“ Agent loaded: {agent_config.agent.name}")
    
    async def evaluate_single_game(self, seed: int) -> Dict:
        """
        è¯„ä¼°å•ä¸ªæ¸¸æˆå®ä¾‹
        
        Args:
            seed: æ¸¸æˆç§å­
            
        Returns:
            æ¸¸æˆç»“æœå­—å…¸
        """
        try:
            result = await self.adapter.play_game(self.agent, seed)
            
            return {
                'seed': seed,
                'score': result.get('score', 0),  # è¿™æ˜¯å…³é”®æŒ‡æ ‡
                'success': result.get('success', False),
                'response_time': result.get('response_time', 0),
                'action': result.get('action', ''),
                'response': result.get('response', ''),
            }
        
        except Exception as e:
            logger.error(f"Game seed {seed} failed: {e}")
            return {
                'seed': seed,
                'score': 0,
                'success': False,
                'response_time': 0,
                'error': str(e)
            }
    
    async def evaluate_all(self) -> Dict:
        """
        è¯„ä¼°æ‰€æœ‰æ¸¸æˆå®ä¾‹
        
        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœ
        """
        logger.info("=" * 70)
        logger.info("  KORGym Word Puzzle - Paper Aligned Evaluation")
        logger.info("=" * 70)
        logger.info(f"Game: {self.game_name}")
        logger.info(f"Agent: {self.agent_config_name}")
        logger.info(f"Number of games: {self.num_seeds}")
        logger.info(f"Level: {self.level}")
        logger.info("")
        
        results = []
        start_time = time.time()
        
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦
        for seed in tqdm(range(self.num_seeds), desc="Evaluating"):
            result = await self.evaluate_single_game(seed)
            results.append(result)
            
            # æ˜¾ç¤ºå½“å‰è¿›åº¦
            if (seed + 1) % 10 == 0:
                current_scores = [r['score'] for r in results]
                current_avg = np.mean(current_scores)
                tqdm.write(f"  Progress: {seed + 1}/{self.num_seeds}, "
                          f"Current avg score: {current_avg:.4f}")
        
        total_time = time.time() - start_time
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        scores = [r['score'] for r in results]
        avg_score = np.mean(scores)
        std_score = np.std(scores)
        max_score = np.max(scores)
        min_score = np.min(scores)
        
        # è®¡ç®—ä¸åŒåˆ†æ•°åŒºé—´çš„åˆ†å¸ƒ
        score_distribution = {
            '0.0': sum(1 for s in scores if s == 0),
            '0.0-0.2': sum(1 for s in scores if 0 < s <= 0.2),
            '0.2-0.4': sum(1 for s in scores if 0.2 < s <= 0.4),
            '0.4-0.6': sum(1 for s in scores if 0.4 < s <= 0.6),
            '0.6-0.8': sum(1 for s in scores if 0.6 < s <= 0.8),
            '0.8-1.0': sum(1 for s in scores if 0.8 < s <= 1.0),
        }
        
        summary = {
            'agent_config': self.agent_config_name,
            'game_name': self.game_name,
            'num_games': self.num_seeds,
            'level': self.level,
            'avg_score': float(avg_score),  # è¿™æ˜¯è®ºæ–‡è¡¨æ ¼ä¸­çš„æ•°å€¼
            'std_score': float(std_score),
            'max_score': float(max_score),
            'min_score': float(min_score),
            'score_distribution': score_distribution,
            'total_time': total_time,
            'avg_time_per_game': total_time / self.num_seeds,
            'results': results,
        }
        
        return summary
    
    def print_results(self, summary: Dict):
        """
        æ‰“å°è¯„ä¼°ç»“æœï¼ˆè®ºæ–‡æ ¼å¼ï¼‰
        
        Args:
            summary: è¯„ä¼°ç»“æœæ‘˜è¦
        """
        logger.info("")
        logger.info("=" * 70)
        logger.info("  Evaluation Results (Paper Format)")
        logger.info("=" * 70)
        logger.info("")
        
        # å…³é”®æŒ‡æ ‡ï¼ˆä¸è®ºæ–‡è¡¨æ ¼å¯¹åº”ï¼‰
        logger.info("ğŸ“Š Paper Table Metrics:")
        logger.info(f"  Average Score: {summary['avg_score']:.3f}")
        logger.info(f"  â†’ This is the value shown in the paper table!")
        logger.info("")
        
        # è¯¦ç»†ç»Ÿè®¡
        logger.info("ğŸ“ˆ Detailed Statistics:")
        logger.info(f"  Number of games: {summary['num_games']}")
        logger.info(f"  Average score: {summary['avg_score']:.4f}")
        logger.info(f"  Std deviation: {summary['std_score']:.4f}")
        logger.info(f"  Max score: {summary['max_score']:.4f}")
        logger.info(f"  Min score: {summary['min_score']:.4f}")
        logger.info("")
        
        # åˆ†æ•°åˆ†å¸ƒ
        logger.info("ğŸ“Š Score Distribution:")
        for range_str, count in summary['score_distribution'].items():
            percentage = count / summary['num_games'] * 100
            bar = "â–ˆ" * int(percentage / 2)
            logger.info(f"  {range_str:8s}: {bar} {count:3d} ({percentage:5.1f}%)")
        logger.info("")
        
        # æ—¶é—´ç»Ÿè®¡
        logger.info("â±ï¸  Time Statistics:")
        logger.info(f"  Total time: {summary['total_time']:.2f}s")
        logger.info(f"  Avg time per game: {summary['avg_time_per_game']:.2f}s")
        logger.info("")
        
        # ä¸è®ºæ–‡å¯¹æ¯”
        logger.info("ğŸ“– Comparison with Paper (Table 7):")
        paper_scores = {
            'O1-2024-12-17': 0.960,
            'Gemini-2.5-pro-03-25': 0.900,
            'Claude-3.7-thinking': 0.820,
            'DeepSeek-R1': 0.820,
            'O3-mini': 0.880,
            'Claude-3.7': 0.580,
            'DeepSeek-v3-0324': 0.460,
            'GPT-4o': 0.420,
            'Doubao-1.5-thinking-pro': 0.600,
            'Doubao-1.5-pro': 0.120,
        }
        
        logger.info(f"  Your score: {summary['avg_score']:.3f}")
        logger.info("")
        
        # æ‰¾åˆ°æœ€æ¥è¿‘çš„æ¨¡å‹
        closest_model = min(
            paper_scores.items(),
            key=lambda x: abs(x[1] - summary['avg_score'])
        )
        logger.info(f"  Closest to: {closest_model[0]} ({closest_model[1]:.3f})")
        
        # æ’å
        better_than = sum(1 for score in paper_scores.values() 
                         if summary['avg_score'] > score)
        logger.info(f"  Better than {better_than}/{len(paper_scores)} models in the paper")
        logger.info("")
        
        logger.info("=" * 70)
    
    def save_results(self, summary: Dict, output_path: str):
        """
        ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        
        Args:
            summary: è¯„ä¼°ç»“æœæ‘˜è¦
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ Results saved to: {output_file}")
        
        # åŒæ—¶ä¿å­˜ä¸€ä¸ªç®€å•çš„åˆ†æ•°æ–‡ä»¶ï¼ˆæ¨¡ä»¿è®ºæ–‡ï¼‰
        score_file = output_file.parent / "score.txt"
        with open(score_file, 'a', encoding='utf-8') as f:
            f.write(f"{summary['agent_config']}: {summary['avg_score']:.4f}\n")
        
        logger.info(f"âœ“ Score appended to: {score_file}")


async def main():
    parser = argparse.ArgumentParser(
        description="Word Puzzle evaluation aligned with KORGym paper"
    )
    parser.add_argument(
        '--agent_config',
        type=str,
        default='practice/logic_agent_hierarchical_learning_clean',
        help='Agent config name'
    )
    parser.add_argument(
        '--exp_id',
        type=str,
        required=True,
        help='Experiment ID (for output filename)'
    )
    parser.add_argument(
        '--num_seeds',
        type=int,
        default=50,
        help='Number of games to evaluate (paper uses 50)'
    )
    parser.add_argument(
        '--level',
        type=int,
        default=4,
        help='Game difficulty level (1-5)'
    )
    parser.add_argument(
        '--output_dir',
        type=str,
        default='workspace/korgym_paper_aligned',
        help='Output directory'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = PaperAlignedEvaluator(
        agent_config_name=args.agent_config,
        game_name="8-word_puzzle",
        num_seeds=args.num_seeds,
        level=args.level,
    )
    
    # è¿è¡Œè¯„ä¼°
    summary = await evaluator.evaluate_all()
    
    # æ‰“å°ç»“æœ
    evaluator.print_results(summary)
    
    # ä¿å­˜ç»“æœ
    output_path = f"{args.output_dir}/{args.exp_id}_word_puzzle.json"
    evaluator.save_results(summary, output_path)
    
    logger.info("")
    logger.info("âœ… Evaluation completed!")
    logger.info("")
    logger.info(f"ğŸ“Š Your score for the paper table: {summary['avg_score']:.3f}")


if __name__ == "__main__":
    asyncio.run(main())











