"""
åˆ†ææ•°æ®åº“ä¸­çš„ZebraLogicæ•°æ®é›†
ç»Ÿè®¡æ¯ä¸ªé¢˜ç›®çš„ä¿¡æ¯ï¼Œå¹¶å°†è¯¦ç»†å†…å®¹å†™å…¥æ–‡æœ¬æ–‡ä»¶
"""
import json
from collections import Counter, defaultdict
from datetime import datetime
from pathlib import Path

from sqlmodel import select
from tqdm import tqdm

from utu.db import DatasetSample, EvaluationSample
from utu.utils import SQLModelUtils, get_logger

logger = get_logger(__name__)


def analyze_dataset_samples():
    """åˆ†æDatasetSampleè¡¨ä¸­çš„ZebraLogicæ•°æ®"""
    
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†æ DatasetSample è¡¨ä¸­çš„ ZebraLogic æ•°æ®é›†")
    print("="*80 + "\n")
    
    with SQLModelUtils.create_session() as session:
        # æŸ¥è¯¢æ‰€æœ‰åŒ…å« ZebraLogic çš„æ•°æ®
        query = select(DatasetSample).where(
            DatasetSample.dataset.like("%ZebraLogic%")
        )
        samples = session.exec(query).all()
        
        if not samples:
            print("âŒ æ²¡æœ‰æ‰¾åˆ° ZebraLogic ç›¸å…³çš„æ•°æ®")
            return None
        
        print(f"âœ… æ‰¾åˆ° {len(samples)} æ¡ ZebraLogic æ•°æ®\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_count": len(samples),
            "dataset_names": Counter(),
            "source_names": Counter(),
            "level_distribution": Counter(),
            "has_meta": 0,
            "has_topic": 0,
        }
        
        # æŒ‰æ•°æ®é›†åç§°åˆ†ç»„
        dataset_groups = defaultdict(list)
        
        for sample in samples:
            # ç»Ÿè®¡æ•°æ®é›†åç§°
            stats["dataset_names"][sample.dataset] += 1
            dataset_groups[sample.dataset].append(sample)
            
            # ç»Ÿè®¡source
            if sample.source:
                stats["source_names"][sample.source] += 1
            
            # ç»Ÿè®¡éš¾åº¦åˆ†å¸ƒ
            if sample.level is not None:
                stats["level_distribution"][sample.level] += 1
            
            # ç»Ÿè®¡metaå’Œtopic
            if sample.meta:
                stats["has_meta"] += 1
            if sample.topic:
                stats["has_topic"] += 1
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_statistics(stats)
        
        return {
            "samples": samples,
            "stats": stats,
            "dataset_groups": dataset_groups
        }


def analyze_evaluation_samples():
    """åˆ†æEvaluationSampleè¡¨ä¸­çš„ZebraLogicæ•°æ®"""
    
    print("\n" + "="*80)
    print("ğŸ“Š åˆ†æ EvaluationSample è¡¨ä¸­çš„ ZebraLogic è¯„ä¼°æ•°æ®")
    print("="*80 + "\n")
    
    with SQLModelUtils.create_session() as session:
        # æŸ¥è¯¢æ‰€æœ‰åŒ…å« ZebraLogic çš„è¯„ä¼°æ•°æ®
        query = select(EvaluationSample).where(
            EvaluationSample.dataset.like("%ZebraLogic%")
        )
        samples = session.exec(query).all()
        
        if not samples:
            print("â„¹ï¸  æ²¡æœ‰æ‰¾åˆ° ZebraLogic ç›¸å…³çš„è¯„ä¼°æ•°æ®")
            return None
        
        print(f"âœ… æ‰¾åˆ° {len(samples)} æ¡ ZebraLogic è¯„ä¼°æ•°æ®\n")
        
        # ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_count": len(samples),
            "exp_ids": Counter(),
            "stage_distribution": Counter(),
            "level_distribution": Counter(),
            "correct_count": 0,
            "incorrect_count": 0,
            "not_judged": 0,
        }
        
        # æŒ‰å®éªŒIDåˆ†ç»„
        exp_groups = defaultdict(list)
        
        for sample in samples:
            stats["exp_ids"][sample.exp_id] += 1
            exp_groups[sample.exp_id].append(sample)
            
            stats["stage_distribution"][sample.stage] += 1
            
            if sample.level is not None:
                stats["level_distribution"][sample.level] += 1
            
            if sample.correct is not None:
                if sample.correct:
                    stats["correct_count"] += 1
                else:
                    stats["incorrect_count"] += 1
            else:
                stats["not_judged"] += 1
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print_evaluation_statistics(stats)
        
        return {
            "samples": samples,
            "stats": stats,
            "exp_groups": exp_groups
        }


def print_statistics(stats):
    """æ‰“å°DatasetSampleç»Ÿè®¡ä¿¡æ¯"""
    
    print(f"ğŸ“ˆ æ€»é¢˜ç›®æ•°: {stats['total_count']}\n")
    
    print("ğŸ“š æ•°æ®é›†åˆ†å¸ƒ:")
    for dataset_name, count in stats["dataset_names"].most_common():
        print(f"  â€¢ {dataset_name}: {count} é¢˜")
    print()
    
    if stats["source_names"]:
        print("ğŸ”— Source åˆ†å¸ƒ:")
        for source_name, count in stats["source_names"].most_common():
            print(f"  â€¢ {source_name}: {count} é¢˜")
        print()
    
    if stats["level_distribution"]:
        print("ğŸ“Š éš¾åº¦åˆ†å¸ƒ:")
        for level, count in sorted(stats["level_distribution"].items()):
            level_name = get_level_name(level)
            print(f"  â€¢ Level {level} ({level_name}): {count} é¢˜")
        print()
    
    print(f"ğŸ·ï¸  åŒ…å« meta ä¿¡æ¯: {stats['has_meta']} é¢˜")
    print(f"ğŸ·ï¸  åŒ…å« topic ä¿¡æ¯: {stats['has_topic']} é¢˜")
    print()


def print_evaluation_statistics(stats):
    """æ‰“å°EvaluationSampleç»Ÿè®¡ä¿¡æ¯"""
    
    print(f"ğŸ“ˆ æ€»è¯„ä¼°æ ·æœ¬æ•°: {stats['total_count']}\n")
    
    print("ğŸ”¬ å®éªŒIDåˆ†å¸ƒ:")
    for exp_id, count in stats["exp_ids"].most_common(10):  # åªæ˜¾ç¤ºå‰10ä¸ª
        print(f"  â€¢ {exp_id}: {count} æ ·æœ¬")
    if len(stats["exp_ids"]) > 10:
        print(f"  ... è¿˜æœ‰ {len(stats['exp_ids']) - 10} ä¸ªå®éªŒ")
    print()
    
    print("ğŸš¦ Stage åˆ†å¸ƒ:")
    for stage, count in stats["stage_distribution"].items():
        print(f"  â€¢ {stage}: {count} æ ·æœ¬")
    print()
    
    if stats["level_distribution"]:
        print("ğŸ“Š éš¾åº¦åˆ†å¸ƒ:")
        for level, count in sorted(stats["level_distribution"].items()):
            level_name = get_level_name(level)
            print(f"  â€¢ Level {level} ({level_name}): {count} æ ·æœ¬")
        print()
    
    # è®¡ç®—å‡†ç¡®ç‡
    judged_count = stats["correct_count"] + stats["incorrect_count"]
    if judged_count > 0:
        accuracy = (stats["correct_count"] / judged_count) * 100
        print("âœ… è¯„ä¼°ç»“æœ:")
        print(f"  â€¢ æ­£ç¡®: {stats['correct_count']} é¢˜")
        print(f"  â€¢ é”™è¯¯: {stats['incorrect_count']} é¢˜")
        print(f"  â€¢ æœªåˆ¤å®š: {stats['not_judged']} é¢˜")
        print(f"  â€¢ å‡†ç¡®ç‡: {accuracy:.2f}% ({stats['correct_count']}/{judged_count})")
    else:
        print(f"â„¹ï¸  æœªåˆ¤å®š: {stats['not_judged']} é¢˜")
    print()


def get_level_name(level):
    """è·å–éš¾åº¦çº§åˆ«åç§°"""
    level_names = {
        1: "Easy",
        2: "Medium",
        3: "Hard",
    }
    return level_names.get(level, "Unknown")


def write_samples_to_file(dataset_groups, output_file):
    """å°†é¢˜ç›®è¯¦ç»†å†…å®¹å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
    
    print(f"ğŸ“ æ­£åœ¨å†™å…¥é¢˜ç›®è¯¦ç»†å†…å®¹åˆ°æ–‡ä»¶: {output_file}")
    
    with open(output_file, "w", encoding="utf-8") as f:
        # å†™å…¥æ–‡ä»¶å¤´
        f.write("="*100 + "\n")
        f.write("ZebraLogic æ•°æ®é›†é¢˜ç›®è¯¦ç»†å†…å®¹\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*100 + "\n\n")
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„å†™å…¥
        for dataset_name, samples in sorted(dataset_groups.items()):
            f.write("\n" + "="*100 + "\n")
            f.write(f"æ•°æ®é›†: {dataset_name}\n")
            f.write(f"é¢˜ç›®æ•°é‡: {len(samples)}\n")
            f.write("="*100 + "\n\n")
            
            # æŒ‰indexæ’åº
            samples_sorted = sorted(samples, key=lambda x: x.index or 0)
            
            for idx, sample in enumerate(samples_sorted, 1):
                write_sample_detail(f, sample, idx)
    
    print(f"âœ… æˆåŠŸå†™å…¥ {output_file}")


def write_sample_detail(f, sample: DatasetSample, seq_num: int):
    """å†™å…¥å•ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯"""
    
    f.write("-"*100 + "\n")
    f.write(f"é¢˜ç›® #{seq_num}\n")
    f.write("-"*100 + "\n\n")
    
    # åŸºæœ¬ä¿¡æ¯
    f.write(f"æ•°æ®åº“ID: {sample.id}\n")
    f.write(f"æ•°æ®é›†: {sample.dataset}\n")
    f.write(f"ç´¢å¼•: {sample.index}\n")
    if sample.source:
        f.write(f"Source: {sample.source}\n")
    if sample.source_index is not None:
        f.write(f"Sourceç´¢å¼•: {sample.source_index}\n")
    if sample.level is not None:
        f.write(f"éš¾åº¦çº§åˆ«: {sample.level} ({get_level_name(sample.level)})\n")
    if sample.topic:
        f.write(f"ä¸»é¢˜: {sample.topic}\n")
    f.write("\n")
    
    # é¢˜ç›®å†…å®¹
    f.write("ã€é¢˜ç›®ã€‘\n")
    f.write("-"*50 + "\n")
    f.write(sample.question)
    f.write("\n" + "-"*50 + "\n\n")
    
    # ç­”æ¡ˆ
    f.write("ã€ç­”æ¡ˆã€‘\n")
    f.write("-"*50 + "\n")
    if sample.answer:
        # å°è¯•æ ¼å¼åŒ–JSONç­”æ¡ˆ
        try:
            answer_dict = json.loads(sample.answer)
            f.write(json.dumps(answer_dict, indent=2, ensure_ascii=False))
        except:
            f.write(sample.answer)
    else:
        f.write("(æ— ç­”æ¡ˆ)")
    f.write("\n" + "-"*50 + "\n\n")
    
    # Metaä¿¡æ¯
    if sample.meta:
        f.write("ã€Metaä¿¡æ¯ã€‘\n")
        f.write("-"*50 + "\n")
        try:
            meta_formatted = json.dumps(sample.meta, indent=2, ensure_ascii=False)
            f.write(meta_formatted)
        except:
            f.write(str(sample.meta))
        f.write("\n" + "-"*50 + "\n\n")
    
    f.write("\n\n")


def write_evaluation_samples_to_file(exp_groups, output_file):
    """å°†è¯„ä¼°æ ·æœ¬è¯¦ç»†å†…å®¹å†™å…¥æ–‡æœ¬æ–‡ä»¶"""
    
    print(f"ğŸ“ æ­£åœ¨å†™å…¥è¯„ä¼°æ ·æœ¬è¯¦ç»†å†…å®¹åˆ°æ–‡ä»¶: {output_file}")
    
    with open(output_file, "w", encoding="utf-8") as f:
        # å†™å…¥æ–‡ä»¶å¤´
        f.write("="*100 + "\n")
        f.write("ZebraLogic è¯„ä¼°æ ·æœ¬è¯¦ç»†å†…å®¹\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write("="*100 + "\n\n")
        
        # æŒ‰å®éªŒIDåˆ†ç»„å†™å…¥
        for exp_id, samples in sorted(exp_groups.items()):
            f.write("\n" + "="*100 + "\n")
            f.write(f"å®éªŒID: {exp_id}\n")
            f.write(f"æ ·æœ¬æ•°é‡: {len(samples)}\n")
            f.write("="*100 + "\n\n")
            
            # æŒ‰dataset_indexæ’åº
            samples_sorted = sorted(samples, key=lambda x: (x.dataset_index or 0, x.id))
            
            for idx, sample in enumerate(samples_sorted, 1):
                write_evaluation_sample_detail(f, sample, idx)
    
    print(f"âœ… æˆåŠŸå†™å…¥ {output_file}")


def write_evaluation_sample_detail(f, sample: EvaluationSample, seq_num: int):
    """å†™å…¥å•ä¸ªè¯„ä¼°æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯"""
    
    f.write("-"*100 + "\n")
    f.write(f"è¯„ä¼°æ ·æœ¬ #{seq_num}\n")
    f.write("-"*100 + "\n\n")
    
    # åŸºæœ¬ä¿¡æ¯
    f.write(f"æ•°æ®åº“ID: {sample.id}\n")
    f.write(f"å®éªŒID: {sample.exp_id}\n")
    f.write(f"æ•°æ®é›†: {sample.dataset}\n")
    f.write(f"æ•°æ®é›†ç´¢å¼•: {sample.dataset_index}\n")
    if sample.source:
        f.write(f"Source: {sample.source}\n")
    if sample.level is not None:
        f.write(f"éš¾åº¦çº§åˆ«: {sample.level} ({get_level_name(sample.level)})\n")
    f.write(f"Stage: {sample.stage}\n")
    
    # è¯„ä¼°ç»“æœ
    if sample.correct is not None:
        result = "âœ… æ­£ç¡®" if sample.correct else "âŒ é”™è¯¯"
        f.write(f"è¯„ä¼°ç»“æœ: {result}\n")
        if sample.reward is not None:
            f.write(f"å¥–åŠ±åˆ†æ•°: {sample.reward}\n")
    
    if sample.time_cost is not None:
        f.write(f"è€—æ—¶: {sample.time_cost:.2f} ç§’\n")
    
    if sample.trace_id:
        f.write(f"Trace ID: {sample.trace_id}\n")
    
    f.write("\n")
    
    # é¢˜ç›®å†…å®¹
    f.write("ã€åŸå§‹é¢˜ç›®ã€‘\n")
    f.write("-"*50 + "\n")
    f.write(sample.raw_question)
    f.write("\n" + "-"*50 + "\n\n")
    
    # å¢å¼ºé¢˜ç›®
    if sample.augmented_question and sample.augmented_question != sample.raw_question:
        f.write("ã€å¢å¼ºé¢˜ç›®ã€‘\n")
        f.write("-"*50 + "\n")
        f.write(sample.augmented_question)
        f.write("\n" + "-"*50 + "\n\n")
    
    # æ­£ç¡®ç­”æ¡ˆ
    f.write("ã€æ­£ç¡®ç­”æ¡ˆã€‘\n")
    f.write("-"*50 + "\n")
    if sample.correct_answer:
        try:
            answer_dict = json.loads(sample.correct_answer)
            f.write(json.dumps(answer_dict, indent=2, ensure_ascii=False))
        except:
            f.write(sample.correct_answer)
    else:
        f.write("(æ— ç­”æ¡ˆ)")
    f.write("\n" + "-"*50 + "\n\n")
    
    # æ¨¡å‹è¾“å‡º
    if sample.response:
        f.write("ã€æ¨¡å‹è¾“å‡ºã€‘\n")
        f.write("-"*50 + "\n")
        # åªæ˜¾ç¤ºå‰1000ä¸ªå­—ç¬¦ï¼Œé¿å…æ–‡ä»¶è¿‡å¤§
        response_preview = sample.response[:1000]
        if len(sample.response) > 1000:
            response_preview += f"\n\n... (æ€»é•¿åº¦: {len(sample.response)} å­—ç¬¦ï¼Œå·²æˆªæ–­)"
        f.write(response_preview)
        f.write("\n" + "-"*50 + "\n\n")
    
    # æå–çš„ç­”æ¡ˆ
    if sample.extracted_final_answer:
        f.write("ã€æå–çš„ç­”æ¡ˆã€‘\n")
        f.write("-"*50 + "\n")
        try:
            extracted_dict = json.loads(sample.extracted_final_answer)
            f.write(json.dumps(extracted_dict, indent=2, ensure_ascii=False))
        except:
            f.write(sample.extracted_final_answer)
        f.write("\n" + "-"*50 + "\n\n")
    
    # åˆ¤å®šæ¨ç†
    if sample.reasoning:
        f.write("ã€åˆ¤å®šæ¨ç†ã€‘\n")
        f.write("-"*50 + "\n")
        f.write(sample.reasoning)
        f.write("\n" + "-"*50 + "\n\n")
    
    f.write("\n\n")


def export_statistics_to_json(dataset_result, evaluation_result, output_file):
    """å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°JSONæ–‡ä»¶"""
    
    print(f"ğŸ“Š æ­£åœ¨å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°: {output_file}")
    
    export_data = {
        "generated_at": datetime.now().isoformat(),
        "dataset_samples": None,
        "evaluation_samples": None,
    }
    
    if dataset_result:
        export_data["dataset_samples"] = {
            "total_count": dataset_result["stats"]["total_count"],
            "dataset_names": dict(dataset_result["stats"]["dataset_names"]),
            "source_names": dict(dataset_result["stats"]["source_names"]),
            "level_distribution": dict(dataset_result["stats"]["level_distribution"]),
            "has_meta": dataset_result["stats"]["has_meta"],
            "has_topic": dataset_result["stats"]["has_topic"],
        }
    
    if evaluation_result:
        export_data["evaluation_samples"] = {
            "total_count": evaluation_result["stats"]["total_count"],
            "exp_ids": dict(evaluation_result["stats"]["exp_ids"]),
            "stage_distribution": dict(evaluation_result["stats"]["stage_distribution"]),
            "level_distribution": dict(evaluation_result["stats"]["level_distribution"]),
            "correct_count": evaluation_result["stats"]["correct_count"],
            "incorrect_count": evaluation_result["stats"]["incorrect_count"],
            "not_judged": evaluation_result["stats"]["not_judged"],
        }
        
        # è®¡ç®—å‡†ç¡®ç‡
        judged = export_data["evaluation_samples"]["correct_count"] + \
                 export_data["evaluation_samples"]["incorrect_count"]
        if judged > 0:
            export_data["evaluation_samples"]["accuracy"] = \
                export_data["evaluation_samples"]["correct_count"] / judged
    
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(export_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… æˆåŠŸå¯¼å‡ºç»Ÿè®¡ä¿¡æ¯")


def main():
    """ä¸»å‡½æ•°"""
    
    print("\n" + "ğŸ¦“"*40)
    print("ZebraLogic æ•°æ®é›†åˆ†æå·¥å…·")
    print("ğŸ¦“"*40 + "\n")
    
    # æ£€æŸ¥æ•°æ®åº“è¿æ¥
    if not SQLModelUtils.check_db_available():
        print("âŒ æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    print("âœ… æ•°æ®åº“è¿æ¥æˆåŠŸ\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("analysis/zebra_dataset")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # åˆ†æDatasetSampleè¡¨
    dataset_result = analyze_dataset_samples()
    
    # åˆ†æEvaluationSampleè¡¨
    evaluation_result = analyze_evaluation_samples()
    
    # å†™å…¥æ–‡ä»¶
    if dataset_result:
        dataset_file = output_dir / f"zebra_dataset_samples_{timestamp}.txt"
        write_samples_to_file(dataset_result["dataset_groups"], dataset_file)
    
    if evaluation_result:
        evaluation_file = output_dir / f"zebra_evaluation_samples_{timestamp}.txt"
        write_evaluation_samples_to_file(evaluation_result["exp_groups"], evaluation_file)
    
    # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯åˆ°JSON
    stats_file = output_dir / f"zebra_statistics_{timestamp}.json"
    export_statistics_to_json(dataset_result, evaluation_result, stats_file)
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼")
    print("="*80)
    print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®: {output_dir.absolute()}")
    if dataset_result:
        print(f"  â€¢ æ•°æ®é›†æ ·æœ¬: zebra_dataset_samples_{timestamp}.txt")
    if evaluation_result:
        print(f"  â€¢ è¯„ä¼°æ ·æœ¬: zebra_evaluation_samples_{timestamp}.txt")
    print(f"  â€¢ ç»Ÿè®¡ä¿¡æ¯: zebra_statistics_{timestamp}.json")
    print()


if __name__ == "__main__":
    main()
































