"""
æŸ¥çœ‹ ZebraLogic å®éªŒçš„è¯„ä¼°ç»“æœ
View ZebraLogic experiment evaluation results

ç”¨æ³• / Usage:
    python scripts/view_zebralogic_results.py
"""

import sys
from pathlib import Path
from sqlmodel import select
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent.parent))

from utu.db import EvaluationSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def calculate_pass_at_k(samples, k=None):
    """è®¡ç®— Pass@K æŒ‡æ ‡"""
    problem_to_scores = defaultdict(list)
    
    for sample in samples:
        reward = sample.reward if sample.reward is not None else 0.0
        problem_to_scores[sample.raw_question].append(reward)
    
    if not problem_to_scores:
        return 0.0, 0, 0
    
    # å¦‚æœæœªæŒ‡å®š kï¼Œä½¿ç”¨æœ€å¤§çš„ k
    if k is None:
        k = max(len(scores) for scores in problem_to_scores.values())
    
    problem_to_max_score = {
        problem: max((s for s in scores if s is not None), default=0.0) 
        for problem, scores in problem_to_scores.items()
    }
    
    pass_k = sum(max_reward for max_reward in problem_to_max_score.values()) / len(problem_to_max_score)
    total_problems = len(problem_to_max_score)
    solved_problems = sum(1 for score in problem_to_max_score.values() if score > 0)
    
    return pass_k, solved_problems, total_problems


def calculate_accuracy(samples):
    """è®¡ç®—å‡†ç¡®ç‡"""
    if not samples:
        return 0.0, 0, 0
    
    correct = sum(1 for s in samples if s.reward and s.reward > 0)
    total = len(samples)
    accuracy = correct / total if total > 0 else 0.0
    
    return accuracy, correct, total


def view_experiment_results(exp_id):
    """æŸ¥çœ‹ç‰¹å®šå®éªŒçš„ç»“æœ"""
    with SQLModelUtils.create_session() as session:
        samples = session.exec(
            select(EvaluationSample).where(EvaluationSample.exp_id == exp_id)
        ).all()
        
        if not samples:
            print(f"âŒ æœªæ‰¾åˆ°å®éªŒ '{exp_id}' çš„è¯„ä¼°ç»“æœ")
            print(f"\nè¯·å…ˆè¿è¡Œè¯„ä¼°:")
            print(f"  uv run python scripts/run_eval.py --config eval/logic/logic_zebralogic_test.yaml")
            return None
        
        # è®¡ç®—æŒ‡æ ‡
        pass_k, solved, total_problems = calculate_pass_at_k(samples)
        accuracy, correct, total_samples = calculate_accuracy(samples)
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\n{'=' * 70}")
        print(f"å®éªŒ ID: {exp_id}")
        print(f"{'=' * 70}")
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"  æ€»é—®é¢˜æ•°: {total_problems}")
        print(f"  æ¯é¢˜é‡‡æ ·æ•°: {total_samples // total_problems if total_problems > 0 else 0}")
        
        print(f"\nâœ… Pass@K æŒ‡æ ‡:")
        k = total_samples // total_problems if total_problems > 0 else 1
        print(f"  Pass@{k}: {pass_k:.4f} ({pass_k*100:.2f}%)")
        print(f"  å·²è§£å†³é—®é¢˜: {solved}/{total_problems}")
        
        print(f"\nğŸ“ˆ å‡†ç¡®ç‡:")
        print(f"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"  æ­£ç¡®æ ·æœ¬: {correct}/{total_samples}")
        
        # æŒ‰é—®é¢˜ç»Ÿè®¡
        print(f"\nğŸ“ æŒ‰é—®é¢˜ç»Ÿè®¡:")
        problem_to_samples = defaultdict(list)
        for sample in samples:
            problem_to_samples[sample.raw_question].append(sample)
        
        solved_problems_list = []
        unsolved_problems_list = []
        
        for problem, prob_samples in problem_to_samples.items():
            max_reward = max((s.reward for s in prob_samples if s.reward is not None), default=0.0)
            if max_reward > 0:
                solved_problems_list.append((problem[:80], max_reward))
            else:
                unsolved_problems_list.append(problem[:80])
        
        if solved_problems_list:
            print(f"\n  âœ“ å·²è§£å†³ ({len(solved_problems_list)} ä¸ª):")
            for i, (problem, reward) in enumerate(solved_problems_list[:5], 1):
                print(f"    {i}. {problem}... (reward: {reward})")
            if len(solved_problems_list) > 5:
                print(f"    ... è¿˜æœ‰ {len(solved_problems_list) - 5} ä¸ªé—®é¢˜")
        
        if unsolved_problems_list:
            print(f"\n  âœ— æœªè§£å†³ ({len(unsolved_problems_list)} ä¸ª):")
            for i, problem in enumerate(unsolved_problems_list[:5], 1):
                print(f"    {i}. {problem}...")
            if len(unsolved_problems_list) > 5:
                print(f"    ... è¿˜æœ‰ {len(unsolved_problems_list) - 5} ä¸ªé—®é¢˜")
        
        print(f"\n{'=' * 70}\n")
        
        return {
            'exp_id': exp_id,
            'pass_k': pass_k,
            'accuracy': accuracy,
            'solved': solved,
            'total_problems': total_problems,
            'correct': correct,
            'total_samples': total_samples,
        }


def compare_baseline_and_practice():
    """æ¯”è¾ƒ baseline å’Œ practice çš„ç»“æœ"""
    baseline_exp_id = "logic_zebralogic_test_eval"
    practice_exp_id = "logic_practice_zebralogic_test_eval"
    
    print("\n" + "=" * 70)
    print("ZebraLogic å®éªŒç»“æœå¯¹æ¯”")
    print("Baseline vs Practice Comparison")
    print("=" * 70)
    
    baseline_results = view_experiment_results(baseline_exp_id)
    practice_results = view_experiment_results(practice_exp_id)
    
    if baseline_results and practice_results:
        print("\n" + "=" * 70)
        print("ğŸ“Š å¯¹æ¯”æ€»ç»“")
        print("=" * 70)
        
        baseline_pass = baseline_results['pass_k']
        practice_pass = practice_results['pass_k']
        improvement = practice_pass - baseline_pass
        improvement_pct = (improvement / baseline_pass * 100) if baseline_pass > 0 else 0
        
        print(f"\nPass@K å¯¹æ¯”:")
        print(f"  Baseline:  {baseline_pass:.4f} ({baseline_pass*100:.2f}%)")
        print(f"  Practice:  {practice_pass:.4f} ({practice_pass*100:.2f}%)")
        print(f"  æå‡:      {improvement:+.4f} ({improvement_pct:+.2f}%)")
        
        baseline_acc = baseline_results['accuracy']
        practice_acc = practice_results['accuracy']
        acc_improvement = practice_acc - baseline_acc
        acc_improvement_pct = (acc_improvement / baseline_acc * 100) if baseline_acc > 0 else 0
        
        print(f"\nå‡†ç¡®ç‡å¯¹æ¯”:")
        print(f"  Baseline:  {baseline_acc:.4f} ({baseline_acc*100:.2f}%)")
        print(f"  Practice:  {practice_acc:.4f} ({practice_acc*100:.2f}%)")
        print(f"  æå‡:      {acc_improvement:+.4f} ({acc_improvement_pct:+.2f}%)")
        
        baseline_solved = baseline_results['solved']
        practice_solved = practice_results['solved']
        total = baseline_results['total_problems']
        
        print(f"\nè§£å†³é—®é¢˜æ•°å¯¹æ¯”:")
        print(f"  Baseline:  {baseline_solved}/{total}")
        print(f"  Practice:  {practice_solved}/{total}")
        print(f"  æ–°å¢è§£å†³:  {practice_solved - baseline_solved} ä¸ªé—®é¢˜")
        
        print(f"\n{'=' * 70}\n")


def list_all_experiments():
    """åˆ—å‡ºæ‰€æœ‰è¯„ä¼°å®éªŒ"""
    with SQLModelUtils.create_session() as session:
        exp_ids = session.exec(
            select(EvaluationSample.exp_id).distinct()
        ).all()
        
        print("\n" + "=" * 70)
        print("æ‰€æœ‰è¯„ä¼°å®éªŒ")
        print("=" * 70)
        
        if exp_ids:
            for exp_id in sorted(exp_ids):
                count = session.exec(
                    select(EvaluationSample).where(EvaluationSample.exp_id == exp_id)
                ).all()
                print(f"  - {exp_id} ({len(count)} samples)")
        else:
            print("  (æ— è¯„ä¼°å®éªŒ)")
        
        print("=" * 70 + "\n")


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æŸ¥çœ‹ ZebraLogic å®éªŒè¯„ä¼°ç»“æœ")
    parser.add_argument(
        "--exp_id",
        type=str,
        help="æŸ¥çœ‹ç‰¹å®šå®éªŒçš„ç»“æœï¼ˆé»˜è®¤: logic_zebralogic_test_evalï¼‰",
        default="logic_zebralogic_test_eval"
    )
    parser.add_argument(
        "--compare",
        action="store_true",
        help="æ¯”è¾ƒ baseline å’Œ practice çš„ç»“æœ"
    )
    parser.add_argument(
        "--list",
        action="store_true",
        help="åˆ—å‡ºæ‰€æœ‰å®éªŒ"
    )
    
    args = parser.parse_args()
    
    if args.list:
        list_all_experiments()
    elif args.compare:
        compare_baseline_and_practice()
    else:
        view_experiment_results(args.exp_id)


if __name__ == "__main__":
    main()

