"""
è¯Šæ–­ ZebraLogic è¯„ä¼°é—®é¢˜
Diagnose ZebraLogic evaluation issues
"""

import sys
from pathlib import Path
from sqlmodel import select

sys.path.insert(0, str(Path(__file__).parent.parent))

from utu.db import EvaluationSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def diagnose_evaluation(exp_id="logic_zebralogic_test_eval"):
    """è¯Šæ–­è¯„ä¼°ç»“æœ"""
    with SQLModelUtils.create_session() as session:
        samples = session.exec(
            select(EvaluationSample).where(EvaluationSample.exp_id == exp_id)
        ).all()
        
        if not samples:
            print(f"âŒ æœªæ‰¾åˆ°å®éªŒ '{exp_id}' çš„æ•°æ®")
            return
        
        print(f"\n{'=' * 70}")
        print(f"è¯Šæ–­å®éªŒ: {exp_id}")
        print(f"{'=' * 70}\n")
        
        # æ£€æŸ¥æ€»ä½“æƒ…å†µ
        print(f"ğŸ“Š æ€»ä½“æƒ…å†µ:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(samples)}")
        
        # æ£€æŸ¥ reward åˆ†å¸ƒ
        reward_none = sum(1 for s in samples if s.reward is None)
        reward_zero = sum(1 for s in samples if s.reward == 0.0)
        reward_positive = sum(1 for s in samples if s.reward and s.reward > 0)
        
        print(f"\nğŸ“ˆ Reward åˆ†å¸ƒ:")
        print(f"  reward = None:     {reward_none} ({reward_none/len(samples)*100:.1f}%)")
        print(f"  reward = 0.0:      {reward_zero} ({reward_zero/len(samples)*100:.1f}%)")
        print(f"  reward > 0:        {reward_positive} ({reward_positive/len(samples)*100:.1f}%)")
        
        # æ£€æŸ¥ correct å­—æ®µ
        correct_true = sum(1 for s in samples if s.correct is True)
        correct_false = sum(1 for s in samples if s.correct is False)
        correct_none = sum(1 for s in samples if s.correct is None)
        
        print(f"\nâœ… Correct åˆ†å¸ƒ:")
        print(f"  correct = True:    {correct_true} ({correct_true/len(samples)*100:.1f}%)")
        print(f"  correct = False:   {correct_false} ({correct_false/len(samples)*100:.1f}%)")
        print(f"  correct = None:    {correct_none} ({correct_none/len(samples)*100:.1f}%)")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å“åº”
        no_response = sum(1 for s in samples if not s.response)
        
        print(f"\nğŸ“ å“åº”æƒ…å†µ:")
        print(f"  æœ‰å“åº”:            {len(samples) - no_response}")
        print(f"  æ— å“åº”:            {no_response}")
        
        # æŠ½æ ·æ£€æŸ¥å‡ ä¸ªæ ·æœ¬
        print(f"\nğŸ” æ ·æœ¬æŠ½æŸ¥ (å‰ 3 ä¸ª):")
        for i, sample in enumerate(samples[:3], 1):
            print(f"\n--- æ ·æœ¬ {i} ---")
            print(f"é—®é¢˜: {sample.raw_question[:100]}...")
            print(f"æ­£ç¡®ç­”æ¡ˆ: {sample.correct_answer[:100] if sample.correct_answer else 'None'}...")
            print(f"æ¨¡å‹å“åº”: {sample.response[:200] if sample.response else 'None'}...")
            print(f"Reward: {sample.reward}")
            print(f"Correct: {sample.correct}")
            print(f"Judged Response: {sample.judged_response}")
        
        # æ£€æŸ¥åˆ¤æ–­å“åº”
        judged_correct = sum(1 for s in samples if s.judged_response == "Correct")
        judged_incorrect = sum(1 for s in samples if s.judged_response == "Incorrect")
        judged_other = len(samples) - judged_correct - judged_incorrect
        
        print(f"\nğŸ“Š Judged Response åˆ†å¸ƒ:")
        print(f"  'Correct':         {judged_correct} ({judged_correct/len(samples)*100:.1f}%)")
        print(f"  'Incorrect':       {judged_incorrect} ({judged_incorrect/len(samples)*100:.1f}%)")
        print(f"  å…¶ä»–:              {judged_other} ({judged_other/len(samples)*100:.1f}%)")
        
        print(f"\n{'=' * 70}\n")
        
        # è¯Šæ–­é—®é¢˜
        print("ğŸ”§ é—®é¢˜è¯Šæ–­:")
        
        if reward_positive == 0:
            print("\nâŒ é—®é¢˜: æ‰€æœ‰ reward éƒ½æ˜¯ 0 æˆ– None")
            print("   å¯èƒ½åŸå› :")
            print("   1. éªŒè¯å‡½æ•°åŠ è½½å¤±è´¥ï¼ˆä½¿ç”¨äº† LLM judgeï¼‰")
            print("   2. æ¨¡å‹è¾“å‡ºæ ¼å¼ä¸åŒ¹é…éªŒè¯å‡½æ•°æœŸæœ›")
            print("   3. éªŒè¯å‡½æ•°é€»è¾‘æœ‰é—®é¢˜")
            print("   4. Ground truth æ ¼å¼ä¸åŒ¹é…")
            
            print("\n   å»ºè®®æ’æŸ¥:")
            print("   1. æ£€æŸ¥è¯„ä¼°æ—¥å¿—ä¸­æ˜¯å¦æœ‰ 'Successfully loaded verification function'")
            print("   2. æŸ¥çœ‹æ¨¡å‹å“åº”æ ¼å¼æ˜¯å¦åŒ…å« \\boxed{} æˆ– <answer> æ ‡ç­¾")
            print("   3. æ£€æŸ¥ ground truth æ˜¯å¦æ˜¯ JSON å­—ç¬¦ä¸²")
            print("   4. æ‰‹åŠ¨æµ‹è¯•éªŒè¯å‡½æ•°:")
            print("      uv run python scripts/test_logic_verifier.py")
        
        if no_response > 0:
            print(f"\nâš ï¸  æœ‰ {no_response} ä¸ªæ ·æœ¬æ²¡æœ‰æ¨¡å‹å“åº”")
            print("   å¯èƒ½åŸå› : æ¨¡å‹ API è°ƒç”¨å¤±è´¥æˆ–è¶…æ—¶")
        
        if reward_none > 0:
            print(f"\nâš ï¸  æœ‰ {reward_none} ä¸ªæ ·æœ¬çš„ reward æ˜¯ None")
            print("   å¯èƒ½åŸå› : éªŒè¯å‡½æ•°è¿”å›äº† None æˆ–åˆ¤æ–­è¿‡ç¨‹å¤±è´¥")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="è¯Šæ–­ ZebraLogic è¯„ä¼°é—®é¢˜")
    parser.add_argument(
        "--exp_id",
        type=str,
        default="logic_zebralogic_test_eval",
        help="å®éªŒ ID"
    )
    
    args = parser.parse_args()
    diagnose_evaluation(args.exp_id)

