#!/usr/bin/env python3
"""
æµ‹è¯• logic.py éªŒè¯å™¨å¯¹ç‰¹å®šç»éªŒåº“ç”Ÿæˆçš„ç­”æ¡ˆçš„åˆ¤æ–­èƒ½åŠ›

æ­¤è„šæœ¬ä»æ•°æ®åº“ä¸­æå–å®é™…çš„ agent ç­”æ¡ˆï¼Œç„¶åæµ‹è¯•éªŒè¯å™¨çš„åˆ¤æ–­æ˜¯å¦æ­£ç¡®
"""

import json
import sys
from pathlib import Path
from typing import List, Dict, Any
from collections import defaultdict

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from utu.db import EvaluationSample, DatasetSample
from utu.practice.verify.logic import verify_func
from utu.utils.sqlmodel_utils import SQLModelUtils
from sqlmodel import Session, select


def get_experiment_samples(exp_id: str, limit: int = 50) -> List[EvaluationSample]:
    """ä»æ•°æ®åº“ä¸­è·å–æŒ‡å®šå®éªŒçš„æ ·æœ¬"""
    engine = SQLModelUtils.get_engine()
    
    with Session(engine) as session:
        stmt = select(EvaluationSample).where(
            EvaluationSample.exp_id == exp_id
        ).limit(limit)
        
        samples = session.exec(stmt).all()
        return list(samples)


def get_experiment_info(exp_id: str) -> Dict[str, Any]:
    """è·å–å®éªŒçš„åŸºæœ¬ä¿¡æ¯ï¼ˆä»æ ·æœ¬ç»Ÿè®¡ï¼‰"""
    engine = SQLModelUtils.get_engine()
    
    with Session(engine) as session:
        stmt = select(EvaluationSample).where(
            EvaluationSample.exp_id == exp_id
        )
        
        samples = session.exec(stmt).all()
        if samples:
            total = len(samples)
            correct = sum(1 for s in samples if s.reward >= 1.0)
            avg_reward = sum(s.reward for s in samples) / total if total > 0 else 0
            
            # è·å–ç¬¬ä¸€ä¸ªæ ·æœ¬çš„åˆ›å»ºæ—¶é—´ä½œä¸ºå‚è€ƒ
            created_at = samples[0].created_at if hasattr(samples[0], 'created_at') else None
            
            return {
                "exp_id": exp_id,
                "agent_config": "N/A",  # EvaluationSample è¡¨ä¸­æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                "total_count": total,
                "correct_count": correct,
                "accuracy": correct / total if total > 0 else 0,
                "avg_reward": avg_reward,
                "created_at": created_at
            }
    return None


def re_verify_samples(samples: List[EvaluationSample]) -> Dict[str, Any]:
    """é‡æ–°éªŒè¯æ‰€æœ‰æ ·æœ¬"""
    results = {
        "total": len(samples),
        "original_correct": 0,
        "reverify_correct": 0,
        "match_count": 0,
        "mismatch_cases": [],
        "verification_details": []
    }
    
    for i, db_sample in enumerate(samples, 1):
        # æ•°æ®åº“ä¸­çš„ EvaluationSample å¯ä»¥ç›´æ¥ç”¨äºéªŒè¯
        # åŸå§‹åˆ¤æ–­
        original_correct = db_sample.reward >= 1.0
        
        # é‡æ–°éªŒè¯
        try:
            verify_result = verify_func(db_sample)
            reverify_correct = verify_result["reward"] >= 1.0
        except Exception as e:
            print(f"âš ï¸  æ ·æœ¬ {i} éªŒè¯å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            verify_result = {"reward": 0.0, "reasoning": f"Verification error: {e}"}
            reverify_correct = False
        
        # ç»Ÿè®¡
        if original_correct:
            results["original_correct"] += 1
        if reverify_correct:
            results["reverify_correct"] += 1
        
        # æ£€æŸ¥æ˜¯å¦åŒ¹é…
        if original_correct == reverify_correct:
            results["match_count"] += 1
        else:
            # è®°å½•ä¸åŒ¹é…çš„æ¡ˆä¾‹
            question_id = getattr(db_sample, 'question_id', 'N/A')
            results["mismatch_cases"].append({
                "sample_id": i,
                "question_id": question_id,
                "original_reward": db_sample.reward,
                "reverify_reward": verify_result["reward"],
                "original_correct": original_correct,
                "reverify_correct": reverify_correct,
                "question": db_sample.raw_question[:200] if db_sample.raw_question else "",
                "response": db_sample.response[:300] if db_sample.response else "",
                "ground_truth": db_sample.correct_answer[:200] if db_sample.correct_answer else ""
            })
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼ˆå‰10ä¸ªï¼‰
        if i <= 10:
            results["verification_details"].append({
                "sample_id": i,
                "original_reward": db_sample.reward,
                "reverify_reward": verify_result["reward"],
                "match": original_correct == reverify_correct
            })
    
    return results


def print_analysis_report(exp_id: str, exp_info: Dict, results: Dict):
    """æ‰“å°åˆ†ææŠ¥å‘Š"""
    print("\n" + "="*80)
    print(f"éªŒè¯å™¨æµ‹è¯•æŠ¥å‘Š: {exp_id}")
    print("="*80)
    
    # å®éªŒä¿¡æ¯
    if exp_info:
        print(f"\nğŸ“Š å®éªŒä¿¡æ¯:")
        print(f"  å®éªŒID: {exp_info['exp_id']}")
        print(f"  Agenté…ç½®: {exp_info['agent_config']}")
        print(f"  åˆ›å»ºæ—¶é—´: {exp_info['created_at']}")
        print(f"  æ€»æ ·æœ¬æ•°: {exp_info['total_count']}")
        print(f"  åŸå§‹æ­£ç¡®æ•°: {exp_info['correct_count']}")
        print(f"  åŸå§‹æ­£ç¡®ç‡: {exp_info['accuracy']:.2%}")
        print(f"  å¹³å‡Reward: {exp_info['avg_reward']:.4f}")
    
    # é‡æ–°éªŒè¯ç»“æœ
    print(f"\nğŸ” é‡æ–°éªŒè¯ç»“æœ:")
    print(f"  æµ‹è¯•æ ·æœ¬æ•°: {results['total']}")
    print(f"  åŸå§‹åˆ¤æ–­ä¸ºæ­£ç¡®: {results['original_correct']} ({results['original_correct']/results['total']*100:.1f}%)")
    print(f"  é‡æ–°éªŒè¯ä¸ºæ­£ç¡®: {results['reverify_correct']} ({results['reverify_correct']/results['total']*100:.1f}%)")
    print(f"  åˆ¤æ–­ä¸€è‡´çš„æ ·æœ¬: {results['match_count']} ({results['match_count']/results['total']*100:.1f}%)")
    print(f"  åˆ¤æ–­ä¸ä¸€è‡´çš„æ ·æœ¬: {len(results['mismatch_cases'])} ({len(results['mismatch_cases'])/results['total']*100:.1f}%)")
    
    # å‰10ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
    print(f"\nğŸ“‹ å‰10ä¸ªæ ·æœ¬çš„éªŒè¯è¯¦æƒ…:")
    for detail in results["verification_details"]:
        status = "âœ“" if detail["match"] else "âœ—"
        print(f"  {status} æ ·æœ¬ {detail['sample_id']}: åŸå§‹={detail['original_reward']:.2f}, é‡éªŒ={detail['reverify_reward']:.2f}")
    
    # ä¸åŒ¹é…æ¡ˆä¾‹åˆ†æ
    if results["mismatch_cases"]:
        print(f"\nâš ï¸  å‘ç° {len(results['mismatch_cases'])} ä¸ªåˆ¤æ–­ä¸ä¸€è‡´çš„æ¡ˆä¾‹:")
        
        for i, case in enumerate(results["mismatch_cases"][:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
            print(f"\n  æ¡ˆä¾‹ {i} (æ ·æœ¬ {case['sample_id']}):")
            print(f"    åŸå§‹åˆ¤æ–­: {'æ­£ç¡®' if case['original_correct'] else 'é”™è¯¯'} (reward={case['original_reward']:.2f})")
            print(f"    é‡æ–°éªŒè¯: {'æ­£ç¡®' if case['reverify_correct'] else 'é”™è¯¯'} (reward={case['reverify_reward']:.2f})")
            print(f"    é—®é¢˜: {case['question'][:150]}...")
            print(f"    ç­”æ¡ˆ: {case['response'][:200]}...")
            print(f"    æ ‡å‡†ç­”æ¡ˆ: {case['ground_truth'][:150]}...")
        
        if len(results["mismatch_cases"]) > 5:
            print(f"\n  ... è¿˜æœ‰ {len(results['mismatch_cases']) - 5} ä¸ªä¸ä¸€è‡´æ¡ˆä¾‹æœªæ˜¾ç¤º")
    
    # ç»“è®º
    print(f"\n" + "="*80)
    print("ğŸ¯ ç»“è®º:")
    print("="*80)
    
    match_rate = results['match_count'] / results['total'] * 100
    
    if match_rate >= 99:
        print("âœ… éªŒè¯å™¨å·¥ä½œæ­£å¸¸ï¼å‡ ä¹æ‰€æœ‰åˆ¤æ–­éƒ½ä¸€è‡´ã€‚")
    elif match_rate >= 95:
        print("âš ï¸  éªŒè¯å™¨åŸºæœ¬æ­£å¸¸ï¼Œä½†å­˜åœ¨å°‘é‡ä¸ä¸€è‡´ï¼ˆå¯èƒ½æ˜¯è¾¹ç•Œæƒ…å†µï¼‰ã€‚")
    elif match_rate >= 80:
        print("âŒ éªŒè¯å™¨å­˜åœ¨æ˜æ˜¾é—®é¢˜ï¼å»ºè®®æ£€æŸ¥éªŒè¯é€»è¾‘ã€‚")
    else:
        print("ğŸš¨ éªŒè¯å™¨ä¸¥é‡å¼‚å¸¸ï¼å¤§é‡åˆ¤æ–­ä¸ä¸€è‡´ï¼")
    
    if results['reverify_correct'] != results['original_correct']:
        diff = results['reverify_correct'] - results['original_correct']
        print(f"\nğŸ’¡ é‡æ–°éªŒè¯å‘ç°:")
        if diff > 0:
            print(f"   {diff} ä¸ªæ ·æœ¬åŸæœ¬åˆ¤æ–­ä¸ºé”™è¯¯ï¼Œä½†é‡æ–°éªŒè¯ä¸ºæ­£ç¡®")
            print(f"   è¿™å¯èƒ½æ„å‘³ç€åŸå§‹è¯„ä¼°æ—¶çš„éªŒè¯é€»è¾‘æœ‰é—®é¢˜")
        else:
            print(f"   {-diff} ä¸ªæ ·æœ¬åŸæœ¬åˆ¤æ–­ä¸ºæ­£ç¡®ï¼Œä½†é‡æ–°éªŒè¯ä¸ºé”™è¯¯")
            print(f"   è¿™å¯èƒ½æ„å‘³ç€å½“å‰éªŒè¯å™¨æ›´ä¸¥æ ¼ï¼Œæˆ–åŸå§‹ç­”æ¡ˆæ ¼å¼æœ‰é—®é¢˜")


def compare_multiple_experiments(exp_ids: List[str], sample_limit: int = 50):
    """å¯¹æ¯”å¤šä¸ªå®éªŒçš„éªŒè¯ç»“æœ"""
    print("\n" + "="*80)
    print("å¤šå®éªŒå¯¹æ¯”åˆ†æ")
    print("="*80)
    
    comparison = []
    
    for exp_id in exp_ids:
        print(f"\nå¤„ç†å®éªŒ: {exp_id}...")
        
        exp_info = get_experiment_info(exp_id)
        if not exp_info:
            print(f"  âš ï¸  æœªæ‰¾åˆ°å®éªŒ {exp_id}")
            continue
        
        samples = get_experiment_samples(exp_id, limit=sample_limit)
        if not samples:
            print(f"  âš ï¸  å®éªŒ {exp_id} æ²¡æœ‰æ ·æœ¬æ•°æ®")
            continue
        
        print(f"  æ‰¾åˆ° {len(samples)} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹é‡æ–°éªŒè¯...")
        results = re_verify_samples(samples)
        
        comparison.append({
            "exp_id": exp_id,
            "agent_config": exp_info.get("agent_config", "N/A"),
            "original_accuracy": exp_info["accuracy"],
            "reverify_accuracy": results["reverify_correct"] / results["total"] if results["total"] > 0 else 0,
            "match_rate": results["match_count"] / results["total"] if results["total"] > 0 else 0,
            "sample_count": results["total"]
        })
    
    # æ‰“å°å¯¹æ¯”è¡¨æ ¼
    if comparison:
        print("\n" + "="*80)
        print("å®éªŒå¯¹æ¯”ç»“æœ")
        print("="*80)
        print(f"\n{'å®éªŒID':<45} {'åŸå§‹æ­£ç¡®ç‡':<12} {'é‡éªŒæ­£ç¡®ç‡':<12} {'åŒ¹é…ç‡':<10} {'æ ·æœ¬æ•°':<8}")
        print("-" * 85)
        
        for c in comparison:
            print(f"{c['exp_id']:<45} {c['original_accuracy']:<12.2%} {c['reverify_accuracy']:<12.2%} {c['match_rate']:<10.2%} {c['sample_count']:<8}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•éªŒè¯å™¨å¯¹ç‰¹å®šç»éªŒåº“ç”Ÿæˆç­”æ¡ˆçš„åˆ¤æ–­èƒ½åŠ›")
    parser.add_argument("--exp_id", type=str, help="å®éªŒID")
    parser.add_argument("--exp_ids", nargs="+", help="å¤šä¸ªå®éªŒIDï¼ˆç”¨äºå¯¹æ¯”ï¼‰")
    parser.add_argument("--sample_limit", type=int, default=50, help="æµ‹è¯•æ ·æœ¬æ•°é‡é™åˆ¶")
    parser.add_argument("--compare", action="store_true", help="å¯¹æ¯”å¤šä¸ªå®éªŒ")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡æœ‰æä¾›ä»»ä½•å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å®éªŒ
    if not args.exp_id and not args.exp_ids:
        print("æœªæŒ‡å®šå®éªŒIDï¼Œä½¿ç”¨ä»¥ä¸‹é»˜è®¤å®éªŒ:")
        args.exp_ids = [
            "zebralogic_practice_medium30_1",
            "zebralogic_baseline_medium30"
        ]
        args.compare = True
        print(f"  {', '.join(args.exp_ids)}")
    
    # å•ä¸ªå®éªŒè¯¦ç»†åˆ†æ
    if args.exp_id:
        print(f"\nå¼€å§‹æµ‹è¯•å®éªŒ: {args.exp_id}")
        
        # è·å–å®éªŒä¿¡æ¯
        exp_info = get_experiment_info(args.exp_id)
        if not exp_info:
            print(f"âŒ æœªæ‰¾åˆ°å®éªŒ {args.exp_id}")
            sys.exit(1)
        
        # è·å–æ ·æœ¬
        samples = get_experiment_samples(args.exp_id, limit=args.sample_limit)
        if not samples:
            print(f"âŒ å®éªŒ {args.exp_id} æ²¡æœ‰æ ·æœ¬æ•°æ®")
            sys.exit(1)
        
        print(f"æ‰¾åˆ° {len(samples)} ä¸ªæ ·æœ¬ï¼Œå¼€å§‹é‡æ–°éªŒè¯...")
        
        # é‡æ–°éªŒè¯
        results = re_verify_samples(samples)
        
        # æ‰“å°æŠ¥å‘Š
        print_analysis_report(args.exp_id, exp_info, results)
    
    # å¤šå®éªŒå¯¹æ¯”
    if args.compare and args.exp_ids:
        compare_multiple_experiments(args.exp_ids, sample_limit=args.sample_limit)
    
    print("\n" + "="*80)
    print("æµ‹è¯•å®Œæˆ")
    print("="*80)


if __name__ == "__main__":
    main()

