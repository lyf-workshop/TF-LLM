#!/usr/bin/env python3
"""æ¯”è¾ƒ ZebraLogic baseline å’Œ enhanced agent çš„ç»“æœ"""

from sqlmodel import select
from collections import defaultdict

from utu.db.eval_datapoint import EvaluationSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def calculate_pass_at_k(samples, k):
    """è®¡ç®— Pass@K"""
    problem_to_samples = defaultdict(list)
    for sample in samples:
        key = sample.raw_question or sample.question
        problem_to_samples[key].append(sample)
    
    solved_problems = 0
    for problem, problem_samples in problem_to_samples.items():
        # å–å‰kä¸ªæ ·æœ¬
        samples_k = problem_samples[:k]
        # å¦‚æœä»»æ„ä¸€ä¸ªæ­£ç¡®ï¼Œåˆ™é—®é¢˜è¢«è§£å†³
        if any(s.reward and s.reward > 0.5 for s in samples_k):
            solved_problems += 1
    
    return solved_problems / len(problem_to_samples) if problem_to_samples else 0.0


def compare_results():
    """æ¯”è¾ƒ baseline å’Œ enhanced ç»“æœ"""
    
    exp_ids = {
        "baseline": "logic_zebralogic_test_eval",
        "enhanced": "logic_practice_zebralogic_test_eval"
    }
    
    print("\n" + "="*80)
    print("ZebraLogic Training-Free GRPO æ•ˆæœå¯¹æ¯”")
    print("="*80 + "\n")
    
    with SQLModelUtils.create_session() as session:
        results = {}
        
        for label, exp_id in exp_ids.items():
            statement = select(EvaluationSample).where(
                EvaluationSample.exp_id == exp_id
            )
            samples = list(session.exec(statement))
            
            if not samples:
                print(f"âš ï¸  {label.upper()}: æœªæ‰¾åˆ°æ•°æ® (exp_id: {exp_id})")
                results[label] = None
                continue
            
            # è®¡ç®—æŒ‡æ ‡
            total_samples = len(samples)
            correct_samples = sum(1 for s in samples if s.reward and s.reward > 0.5)
            accuracy = correct_samples / total_samples if total_samples > 0 else 0.0
            
            # Pass@K (k=32)
            pass_at_32 = calculate_pass_at_k(samples, 32)
            
            # æŒ‰é—®é¢˜åˆ†ç»„ç»Ÿè®¡
            problem_to_samples = defaultdict(list)
            for sample in samples:
                key = sample.raw_question or sample.question
                problem_to_samples[key].append(sample)
            
            total_problems = len(problem_to_samples)
            solved_problems = sum(
                1 for problem_samples in problem_to_samples.values()
                if any(s.reward and s.reward > 0.5 for s in problem_samples)
            )
            
            # ç»Ÿè®¡æ¯ä¸ªé—®é¢˜çš„æ­£ç¡®ç­”æ¡ˆæ•°
            per_problem_stats = []
            for problem, problem_samples in sorted(problem_to_samples.items()):
                correct_in_problem = sum(1 for s in problem_samples if s.reward and s.reward > 0.5)
                per_problem_stats.append({
                    'problem': problem,
                    'total': len(problem_samples),
                    'correct': correct_in_problem,
                    'wrong': len(problem_samples) - correct_in_problem
                })
            
            results[label] = {
                "total_samples": total_samples,
                "correct_samples": correct_samples,
                "accuracy": accuracy,
                "total_problems": total_problems,
                "solved_problems": solved_problems,
                "pass_at_32": pass_at_32,
                "per_problem_stats": per_problem_stats,
            }
        
        # æ˜¾ç¤ºå¯¹æ¯”
        if results["baseline"] and results["enhanced"]:
            print("ğŸ“Š æ€§èƒ½å¯¹æ¯”:\n")
            print(f"{'æŒ‡æ ‡':<20} {'Baseline':<20} {'Enhanced':<20} {'æ”¹è¿›':<20}")
            print("-" * 80)
            
            # Accuracy
            baseline_acc = results["baseline"]["accuracy"]
            enhanced_acc = results["enhanced"]["accuracy"]
            acc_improvement = enhanced_acc - baseline_acc
            print(f"{'Accuracy':<20} {baseline_acc:>8.2%} ({results['baseline']['correct_samples']}/{results['baseline']['total_samples']:<8}) "
                  f"{enhanced_acc:>8.2%} ({results['enhanced']['correct_samples']}/{results['enhanced']['total_samples']:<8}) "
                  f"{acc_improvement:>+8.2%}")
            
            # Pass@32
            baseline_pass = results["baseline"]["pass_at_32"]
            enhanced_pass = results["enhanced"]["pass_at_32"]
            pass_improvement = enhanced_pass - baseline_pass
            print(f"{'Pass@32':<20} {baseline_pass:>8.2%} ({results['baseline']['solved_problems']}/{results['baseline']['total_problems']:<8}) "
                  f"{enhanced_pass:>8.2%} ({results['enhanced']['solved_problems']}/{results['enhanced']['total_problems']:<8}) "
                  f"{pass_improvement:>+8.2%}")
            
            print("\n" + "="*80)
            
            # æ€»ç»“
            if acc_improvement > 0 or pass_improvement > 0:
                print("âœ… Training-Free GRPO å¸¦æ¥äº†æ€§èƒ½æå‡ï¼")
                if acc_improvement > 0:
                    print(f"   - Accuracy æå‡: {acc_improvement:+.2%}")
                if pass_improvement > 0:
                    print(f"   - Pass@32 æå‡: {pass_improvement:+.2%}")
            elif acc_improvement < 0 or pass_improvement < 0:
                print("âš ï¸  æ€§èƒ½ä¸‹é™ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´è®­ç»ƒå‚æ•°")
            else:
                print("â¡ï¸  æ€§èƒ½æŒå¹³")
            
            print("="*80 + "\n")
            
            # æ˜¾ç¤ºæ¯ä¸ªé¢˜ç›®çš„æ­£ç¡®ç­”æ¡ˆæ•°
            print("æ¯ä¸ªé¢˜ç›®çš„æ­£ç¡®ç­”æ¡ˆæ•°ç»Ÿè®¡:\n")
            print(f"{'é¢˜ç›®':<6} {'Baselineæ­£ç¡®æ•°':<18} {'Practiceæ­£ç¡®æ•°':<18} {'å˜åŒ–':<10}")
            print("-" * 60)
            
            baseline_stats = results["baseline"]["per_problem_stats"]
            practice_stats = results["enhanced"]["per_problem_stats"]
            
            # åˆ›å»ºé—®é¢˜åˆ°ç»Ÿè®¡çš„æ˜ å°„
            baseline_map = {s['problem']: s for s in baseline_stats}
            practice_map = {s['problem']: s for s in practice_stats}
            
            all_problems = set(baseline_map.keys()) | set(practice_map.keys())
            
            for i, problem in enumerate(sorted(all_problems), 1):
                baseline_correct = baseline_map.get(problem, {}).get('correct', 0)
                practice_correct = practice_map.get(problem, {}).get('correct', 0)
                change = practice_correct - baseline_correct
                change_str = f"{change:+d}" if change != 0 else "0"
                
                print(f"{i:<6} {baseline_correct}/32{'':<10} {practice_correct}/32{'':<10} {change_str:<10}")
            
            print("="*80 + "\n")
            
        elif results["baseline"] and not results["enhanced"]:
            print("âœ… Baseline è¯„ä¼°å·²å®Œæˆ")
            print(f"   - Accuracy: {results['baseline']['accuracy']:.2%}")
            print(f"   - Pass@32: {results['baseline']['pass_at_32']:.2%}")
            print("\nâ³ è¯·è¿è¡Œ Training-Free GRPO å’Œ Enhanced è¯„ä¼°")
            print("   1. uv run python scripts/run_training_free_GRPO.py --config practice/logic_reasoning_zebralogic.yaml")
            print("   2. uv run python scripts/run_eval.py --config eval/logic/logic_practice_zebralogic_test.yaml")
            print()
            
        elif not results["baseline"] and results["enhanced"]:
            print("âš ï¸  åªæ‰¾åˆ° Enhanced è¯„ä¼°æ•°æ®ï¼Œç¼ºå°‘ Baseline")
            print("è¯·å…ˆè¿è¡Œ Baseline è¯„ä¼°ï¼š")
            print("   uv run python scripts/run_eval.py --config eval/logic/logic_zebralogic_test.yaml")
            print()
            
        else:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°æ•°æ®")
            print("\nè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤è¿è¡Œå®éªŒï¼š")
            print("   1. Baseline: uv run python scripts/run_eval.py --config eval/logic/logic_zebralogic_test.yaml")
            print("   2. GRPO: uv run python scripts/run_training_free_GRPO.py --config practice/logic_reasoning_zebralogic.yaml")
            print("   3. Enhanced: uv run python scripts/run_eval.py --config eval/logic/logic_practice_zebralogic_test.yaml")
            print()


if __name__ == "__main__":
    compare_results()

