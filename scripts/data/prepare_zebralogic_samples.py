#!/usr/bin/env python3
"""
å‡†å¤‡ ZebraLogic æ•°æ®é›†æ ·æœ¬
- è®­ç»ƒé›†ï¼š100é“é¢˜ï¼ˆéš¾åº¦ç¨é«˜ï¼‰
- æµ‹è¯•é›†ï¼š30é“é¢˜ï¼ˆéš¾åº¦ä¸­ç­‰ï¼‰
"""

import argparse
import json
import os
import sys
from pathlib import Path
from typing import List

from datasets import load_dataset

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from sqlmodel import select

from utu.db.eval_datapoint import DatasetSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def load_zebralogic_dataset():
    """åŠ è½½ ZebraLogic æ•°æ®é›†"""
    # æ”¯æŒå¤šç§è·¯å¾„æ ¼å¼ï¼ˆWindows å’Œ WSLï¼‰
    local_paths = [
        "ZebraLogic/grid_mode/test-00000-of-00001.parquet",  # ç›¸å¯¹è·¯å¾„
        "F:\\youtu-agent\\ZebraLogic\\grid_mode\\test-00000-of-00001.parquet",  # Windows
        "/mnt/f/youtu-agent/ZebraLogic/grid_mode/test-00000-of-00001.parquet",  # WSL
    ]
    
    local_file_found = None
    for path in local_paths:
        if os.path.exists(path):
            local_file_found = path
            break
    
    if local_file_found:
        print(f"ğŸ“‚ Loading from local file: {local_file_found}")
        dataset = load_dataset("parquet", data_files=local_file_found, split="train")
    else:
        print("ğŸ“¥ Loading from HuggingFace...")
        dataset = load_dataset("WildEval/ZebraLogic", split="test")
    
    return dataset


def analyze_difficulty(dataset):
    """åˆ†ææ•°æ®é›†éš¾åº¦åˆ†å¸ƒ"""
    print("\n" + "="*60)
    print("ğŸ“Š Dataset Structure Analysis")
    print("="*60)
    
    print(f"\nTotal samples: {len(dataset)}")
    print(f"\nAvailable fields: {list(dataset.features.keys())}")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
    print("\n" + "-"*60)
    print("Sample data (first entry):")
    print("-"*60)
    sample = dataset[0]
    for key, value in sample.items():
        if isinstance(value, str) and len(value) > 200:
            print(f"{key}: {value[:200]}...")
        else:
            print(f"{key}: {value}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰éš¾åº¦å­—æ®µ
    difficulty_fields = ['difficulty', 'level', 'complexity', 'stars', 'size']
    found_difficulty_field = None
    
    for field in difficulty_fields:
        if field in dataset.features:
            found_difficulty_field = field
            break
    
    if found_difficulty_field:
        print(f"\nâœ“ Found difficulty field: '{found_difficulty_field}'")
        # ç»Ÿè®¡éš¾åº¦åˆ†å¸ƒ
        difficulties = [item[found_difficulty_field] for item in dataset]
        from collections import Counter
        difficulty_counts = Counter(difficulties)
        print(f"\nDifficulty distribution:")
        for diff, count in sorted(difficulty_counts.items()):
            print(f"  {diff}: {count} samples")
        return found_difficulty_field
    else:
        print(f"\nâš  No explicit difficulty field found")
        print(f"Available fields for difficulty estimation: {list(dataset.features.keys())}")
        return None


def select_samples_by_difficulty(
    dataset,
    difficulty_field: str = None,
    train_size: int = 100,
    test_size: int = 30,
):
    """
    æ ¹æ®éš¾åº¦é€‰æ‹©æ ·æœ¬
    - è®­ç»ƒé›†ï¼šé«˜éš¾åº¦
    - æµ‹è¯•é›†ï¼šä¸­ç­‰éš¾åº¦
    """
    import random
    
    if difficulty_field and difficulty_field in dataset.features:
        # æŒ‰éš¾åº¦ç­›é€‰
        print(f"\nğŸ“Œ Selecting samples based on '{difficulty_field}' field...")
        
        # è·å–æ‰€æœ‰éš¾åº¦å€¼
        difficulties = list(set([item[difficulty_field] for item in dataset]))
        difficulties_sorted = sorted(difficulties)
        
        print(f"Available difficulty levels: {difficulties_sorted}")
        
        # åˆ†é…éš¾åº¦çº§åˆ«
        if len(difficulties_sorted) >= 3:
            # é«˜éš¾åº¦ï¼šæœ€é«˜çš„å‡ ä¸ªçº§åˆ«
            high_diff = difficulties_sorted[-2:]
            # ä¸­ç­‰éš¾åº¦ï¼šä¸­é—´çº§åˆ«
            mid_diff = difficulties_sorted[len(difficulties_sorted)//2 : len(difficulties_sorted)//2 + 2]
        else:
            # éš¾åº¦çº§åˆ«ä¸å¤Ÿï¼Œä½¿ç”¨ç®€å•ç­–ç•¥
            high_diff = [difficulties_sorted[-1]] if difficulties_sorted else difficulties_sorted
            mid_diff = [difficulties_sorted[0]] if len(difficulties_sorted) > 1 else difficulties_sorted
        
        print(f"\nğŸ”¥ High difficulty levels for training: {high_diff}")
        print(f"âš–ï¸  Medium difficulty levels for testing: {mid_diff}")
        
        # ç­›é€‰æ ·æœ¬
        high_samples = [item for item in dataset if item[difficulty_field] in high_diff]
        mid_samples = [item for item in dataset if item[difficulty_field] in mid_diff]
        
        print(f"\nAvailable samples:")
        print(f"  High difficulty: {len(high_samples)}")
        print(f"  Medium difficulty: {len(mid_samples)}")
        
        # é‡‡æ ·
        if len(high_samples) < train_size:
            print(f"\nâš  Warning: Not enough high difficulty samples ({len(high_samples)} < {train_size})")
            print(f"   Will use all available high difficulty samples")
            train_samples = high_samples
        else:
            train_samples = random.sample(high_samples, train_size)
        
        if len(mid_samples) < test_size:
            print(f"\nâš  Warning: Not enough medium difficulty samples ({len(mid_samples)} < {test_size})")
            print(f"   Will use all available medium difficulty samples")
            test_samples = mid_samples
        else:
            test_samples = random.sample(mid_samples, test_size)
    
    else:
        # æ²¡æœ‰éš¾åº¦å­—æ®µï¼Œéšæœºé‡‡æ ·
        print(f"\nâš  No difficulty field available, using random sampling...")
        all_indices = list(range(len(dataset)))
        random.shuffle(all_indices)
        
        train_indices = all_indices[:train_size]
        test_indices = all_indices[train_size:train_size + test_size]
        
        train_samples = [dataset[i] for i in train_indices]
        test_samples = [dataset[i] for i in test_indices]
    
    return train_samples, test_samples


def save_to_database(samples: List[dict], dataset_name: str, source: str = "training_free_grpo"):
    """ä¿å­˜æ ·æœ¬åˆ°æ•°æ®åº“"""
    with SQLModelUtils.create_session() as session:
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
            existing = session.exec(
                select(DatasetSample).where(DatasetSample.dataset == dataset_name)
            ).first()
            
            if existing:
                print(f"\nâš  Dataset '{dataset_name}' already exists in database!")
                response = input("Do you want to overwrite it? (yes/no): ")
                if response.lower() != 'yes':
                    print("âŒ Aborted")
                    return False
                
                # åˆ é™¤ç°æœ‰æ•°æ®
                existing_all = session.exec(
                    select(DatasetSample).where(DatasetSample.dataset == dataset_name)
                ).all()
                for item in existing_all:
                    session.delete(item)
                session.commit()
                print(f"âœ“ Deleted {len(existing_all)} existing samples")
            
            # æ·»åŠ æ–°æ ·æœ¬
            new_samples = []
            for idx, sample in enumerate(samples):
                # å°† puzzle ä½œä¸º questionï¼Œsolution ä½œä¸º answer
                question = sample.get('puzzle', sample.get('question', ''))
                answer = sample.get('solution', sample.get('answer', ''))
                
                # ç¡®ä¿ question å’Œ answer æ˜¯å­—ç¬¦ä¸²ç±»å‹
                # å¦‚æœ answer æ˜¯å­—å…¸ï¼ˆå¦‚ ZebraLogic çš„ solutionï¼‰ï¼Œè½¬æ¢ä¸º JSON å­—ç¬¦ä¸²
                if isinstance(question, dict):
                    question = json.dumps(question, ensure_ascii=False)
                else:
                    question = str(question) if question else ""
                
                if isinstance(answer, dict):
                    answer = json.dumps(answer, ensure_ascii=False)
                else:
                    answer = str(answer) if answer else ""
                
                db_sample = DatasetSample(
                    dataset=dataset_name,
                    index=idx + 1,  # ç´¢å¼•ä»1å¼€å§‹
                    source=source,
                    question=question,
                    answer=answer,
                )
                new_samples.append(db_sample)
            
            session.add_all(new_samples)
            session.commit()
            print(f"âœ… Successfully saved {len(new_samples)} samples to '{dataset_name}'")
            return True
        
        except Exception as e:
            session.rollback()
            print(f"âŒ Error saving to database: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    parser = argparse.ArgumentParser(description="Prepare ZebraLogic samples for Training-Free GRPO")
    parser.add_argument("--train_size", type=int, default=100, help="Number of training samples")
    parser.add_argument("--test_size", type=int, default=30, help="Number of test samples")
    parser.add_argument("--train_name", type=str, default="ZebraLogic-Train-100", help="Training dataset name")
    parser.add_argument("--test_name", type=str, default="ZebraLogic-Test-30", help="Test dataset name")
    parser.add_argument("--analyze_only", action="store_true", help="Only analyze dataset, don't save")
    
    args = parser.parse_args()
    
    print("ğŸš€ ZebraLogic Dataset Preparation")
    print("="*60)
    
    # 1. åŠ è½½æ•°æ®é›†
    dataset = load_zebralogic_dataset()
    
    # 2. åˆ†æéš¾åº¦åˆ†å¸ƒ
    difficulty_field = analyze_difficulty(dataset)
    
    if args.analyze_only:
        print("\nâœ“ Analysis complete (analyze_only mode)")
        return
    
    # 3. é€‰æ‹©æ ·æœ¬
    print("\n" + "="*60)
    print("ğŸ“ Selecting Samples")
    print("="*60)
    
    train_samples, test_samples = select_samples_by_difficulty(
        dataset,
        difficulty_field=difficulty_field,
        train_size=args.train_size,
        test_size=args.test_size,
    )
    
    print(f"\nâœ“ Selected:")
    print(f"  Training: {len(train_samples)} samples")
    print(f"  Testing: {len(test_samples)} samples")
    
    # 4. ä¿å­˜åˆ°æ•°æ®åº“
    print("\n" + "="*60)
    print("ğŸ’¾ Saving to Database")
    print("="*60)
    
    success = True
    if train_samples:
        success &= save_to_database(train_samples, args.train_name)
    
    if test_samples:
        success &= save_to_database(test_samples, args.test_name)
    
    if success:
        print("\n" + "="*60)
        print("âœ… All Done!")
        print("="*60)
        print(f"\nDatasets created:")
        print(f"  1. {args.train_name}: {len(train_samples)} samples (for training)")
        print(f"  2. {args.test_name}: {len(test_samples)} samples (for evaluation)")
        print(f"\nNext steps:")
        print(f"  1. Update practice config to use '{args.train_name}'")
        print(f"  2. Update eval config to use '{args.test_name}'")
        print(f"  3. Run: uv run python scripts/run_practice.py --config practice/logic_reasoning_zebralogic.yaml")


if __name__ == "__main__":
    main()

