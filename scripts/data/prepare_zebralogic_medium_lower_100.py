#!/usr/bin/env python3
"""
å‡†å¤‡ ZebraLogic ä¸­ä¸‹ç­‰éš¾åº¦100é¢˜æ•°æ®é›†
ä»åŸå§‹1000é“é¢˜ç›®ä¸­é€‰æ‹©ä¸­ä¸‹ç­‰éš¾åº¦çš„100é“é¢˜ç›®ç”¨äºè®­ç»ƒ/è¯„ä¼°
"""

import argparse
import json
import os
import random
import sys
from pathlib import Path
from typing import List, Optional

from datasets import load_dataset

# Add project root to path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from sqlmodel import select

from utu.db.eval_datapoint import DatasetSample
from utu.utils.sqlmodel_utils import SQLModelUtils


def load_zebralogic_dataset():
    """åŠ è½½ ZebraLogic æ•°æ®é›†ï¼ˆåŸå§‹1000é¢˜ï¼‰"""
    print("ğŸ“‚ Loading ZebraLogic dataset...")
    
    # æ”¯æŒå¤šç§è·¯å¾„æ ¼å¼ï¼ˆWindows å’Œ WSLï¼‰
    local_paths = [
        "ZebraLogic/grid_mode/test-00000-of-00001.parquet",  # ç›¸å¯¹è·¯å¾„
        "F:\\youtu-agent\\ZebraLogic\\grid_mode\\test-00000-of-00001.parquet",  # Windows
        "/mnt/f/youtu-agent/ZebraLogic/grid_mode/test-00000-of-00001.parquet",  # WSL
    ]
    
    local_file_found = None
    for path in local_paths:
        if os.path.exists(path):
            local_file_found = path
            break
    
    if local_file_found:
        print(f"âœ“ Loading from local file: {local_file_found}")
        dataset = load_dataset("parquet", data_files=local_file_found, split="train")
    else:
        print("ğŸ“¥ Loading from HuggingFace (this may take a while)...")
        dataset = load_dataset("WildEval/ZebraLogic", split="test")
    
    print(f"âœ“ Loaded {len(dataset)} samples")
    return dataset


def analyze_difficulty(dataset):
    """
    åˆ†ææ•°æ®é›†éš¾åº¦åˆ†å¸ƒ
    
    Returns:
        difficulty_field: éš¾åº¦å­—æ®µåç§°ï¼Œå¦‚æœæ‰¾åˆ°çš„è¯
        difficulty_stats: éš¾åº¦åˆ†å¸ƒç»Ÿè®¡
    """
    print("\n" + "="*70)
    print("ğŸ“Š Dataset Difficulty Analysis")
    print("="*70)
    
    print(f"\nTotal samples: {len(dataset)}")
    print(f"Available fields: {list(dataset.features.keys())}")
    
    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬ï¼ˆéƒ¨åˆ†å†…å®¹ï¼‰
    if len(dataset) > 0:
        print("\n" + "-"*70)
        print("Sample data (first entry - truncated):")
        print("-"*70)
        sample = dataset[0]
        for key, value in sample.items():
            if isinstance(value, str) and len(value) > 100:
                print(f"  {key}: {value[:100]}...")
            elif isinstance(value, dict):
                print(f"  {key}: {str(value)[:100]}...")
            else:
                print(f"  {key}: {value}")
    
    # æ£€æŸ¥å¯èƒ½çš„éš¾åº¦å­—æ®µ
    difficulty_fields = ['difficulty', 'level', 'complexity', 'stars', 'size', 'puzzle_size']
    found_difficulty_field = None
    
    for field in difficulty_fields:
        if field in dataset.features:
            found_difficulty_field = field
            break
    
    difficulty_stats = {}
    
    if found_difficulty_field:
        print(f"\nâœ“ Found difficulty field: '{found_difficulty_field}'")
        
        # ç»Ÿè®¡éš¾åº¦åˆ†å¸ƒ
        from collections import Counter
        difficulties = [item[found_difficulty_field] for item in dataset]
        difficulty_counts = Counter(difficulties)
        
        print(f"\nDifficulty distribution:")
        for diff in sorted(difficulty_counts.keys()):
            count = difficulty_counts[diff]
            percentage = (count / len(dataset)) * 100
            print(f"  {diff}: {count} samples ({percentage:.1f}%)")
        
        difficulty_stats = {
            'field': found_difficulty_field,
            'counts': difficulty_counts,
            'sorted_levels': sorted(difficulty_counts.keys())
        }
    else:
        print(f"\nâš  No explicit difficulty field found")
        print(f"Will estimate difficulty based on available metrics...")
        
        # å°è¯•åŸºäºå…¶ä»–æŒ‡æ ‡ä¼°ç®—éš¾åº¦ï¼ˆå¦‚puzzleé•¿åº¦ç­‰ï¼‰
        if 'puzzle' in dataset.features:
            print("\nEstimating difficulty based on puzzle length...")
            lengths = []
            for item in dataset:
                puzzle = item.get('puzzle', '')
                if isinstance(puzzle, str):
                    lengths.append(len(puzzle))
                elif isinstance(puzzle, dict):
                    lengths.append(len(str(puzzle)))
                else:
                    lengths.append(0)
            
            # æŒ‰é•¿åº¦åˆ†ä¸º3ä¸ªéš¾åº¦çº§åˆ«
            sorted_lengths = sorted(lengths)
            low_threshold = sorted_lengths[len(sorted_lengths) // 3]
            high_threshold = sorted_lengths[2 * len(sorted_lengths) // 3]
            
            print(f"\nEstimated difficulty thresholds (by length):")
            print(f"  Low: < {low_threshold} chars")
            print(f"  Medium: {low_threshold} - {high_threshold} chars")
            print(f"  High: > {high_threshold} chars")
            
            difficulty_stats = {
                'field': 'estimated_difficulty',
                'method': 'puzzle_length',
                'low_threshold': low_threshold,
                'high_threshold': high_threshold
            }
    
    return found_difficulty_field, difficulty_stats


def select_medium_lower_samples(
    dataset,
    difficulty_field: Optional[str] = None,
    difficulty_stats: dict = None,
    num_samples: int = 100,
    seed: int = 42,
):
    """
    é€‰æ‹©ä¸­ä¸‹ç­‰éš¾åº¦çš„æ ·æœ¬
    
    ç­–ç•¥ï¼š
    - å¦‚æœæœ‰æ˜ç¡®çš„éš¾åº¦å­—æ®µï¼šé€‰æ‹©ä¸­ç­‰åä¸‹çš„éš¾åº¦çº§åˆ«
    - å¦‚æœæ²¡æœ‰éš¾åº¦å­—æ®µï¼šåŸºäºpuzzleé•¿åº¦ç­‰æŒ‡æ ‡ä¼°ç®—éš¾åº¦
    
    Args:
        dataset: æ•°æ®é›†
        difficulty_field: éš¾åº¦å­—æ®µåç§°
        difficulty_stats: éš¾åº¦ç»Ÿè®¡ä¿¡æ¯
        num_samples: è¦é€‰æ‹©çš„æ ·æœ¬æ•°é‡
        seed: éšæœºç§å­
    
    Returns:
        selected_samples: é€‰ä¸­çš„æ ·æœ¬åˆ—è¡¨
    """
    random.seed(seed)
    
    print("\n" + "="*70)
    print("ğŸ“ Selecting Medium-Lower Difficulty Samples")
    print("="*70)
    
    if difficulty_field and difficulty_field in dataset.features:
        # æ–¹æ³•1: åŸºäºæ˜ç¡®çš„éš¾åº¦å­—æ®µ
        print(f"Selection strategy: Using '{difficulty_field}' field")
        
        sorted_levels = difficulty_stats['sorted_levels']
        total_levels = len(sorted_levels)
        
        print(f"\nAvailable difficulty levels: {sorted_levels}")
        
        # é€‰æ‹©ä¸­ä¸‹ç­‰éš¾åº¦ï¼šæ ¹æ®éš¾åº¦çº§åˆ«æ•°é‡å†³å®š
        if total_levels >= 5:
            # 5ä¸ªæˆ–æ›´å¤šçº§åˆ«ï¼šé€‰æ‹©ç¬¬2ã€3ä¸ªçº§åˆ«ï¼ˆè·³è¿‡æœ€ç®€å•çš„ï¼‰
            target_levels = sorted_levels[1:3]
        elif total_levels >= 3:
            # 3-4ä¸ªçº§åˆ«ï¼šé€‰æ‹©ç¬¬1ã€2ä¸ªçº§åˆ«
            target_levels = sorted_levels[:2]
        else:
            # 2ä¸ªæˆ–æ›´å°‘çº§åˆ«ï¼šé€‰æ‹©è¾ƒä½çš„çº§åˆ«
            target_levels = [sorted_levels[0]]
        
        print(f"âœ“ Selected difficulty levels for sampling: {target_levels}")
        print(f"  (Medium-lower difficulty range)")
        
        # ç­›é€‰ç›®æ ‡éš¾åº¦çš„æ ·æœ¬
        target_samples = [
            item for item in dataset 
            if item[difficulty_field] in target_levels
        ]
        
        print(f"\nâœ“ Found {len(target_samples)} samples in target difficulty range")
        
        # é‡‡æ ·
        if len(target_samples) <= num_samples:
            print(f"  Using all {len(target_samples)} available samples")
            selected_samples = target_samples
        else:
            print(f"  Randomly sampling {num_samples} from {len(target_samples)} samples")
            selected_samples = random.sample(target_samples, num_samples)
    
    elif difficulty_stats and 'method' in difficulty_stats:
        # æ–¹æ³•2: åŸºäºä¼°ç®—çš„éš¾åº¦
        print(f"Selection strategy: Using estimated difficulty ({difficulty_stats['method']})")
        
        low_threshold = difficulty_stats['low_threshold']
        high_threshold = difficulty_stats['high_threshold']
        
        # é€‰æ‹©ä¸­ç­‰éš¾åº¦ï¼ˆåä¸‹ï¼‰ï¼šç•¥é«˜äºä½é˜ˆå€¼åˆ°ä¸­ä½æ•°
        target_min = low_threshold
        target_max = (low_threshold + high_threshold) / 2
        
        print(f"\nâœ“ Target difficulty range: {target_min:.0f} - {target_max:.0f} chars")
        
        # ç­›é€‰æ ·æœ¬
        target_samples = []
        for item in dataset:
            puzzle = item.get('puzzle', '')
            if isinstance(puzzle, str):
                length = len(puzzle)
            elif isinstance(puzzle, dict):
                length = len(str(puzzle))
            else:
                length = 0
            
            if target_min <= length <= target_max:
                target_samples.append(item)
        
        print(f"\nâœ“ Found {len(target_samples)} samples in target range")
        
        # é‡‡æ ·
        if len(target_samples) <= num_samples:
            print(f"  Using all {len(target_samples)} available samples")
            selected_samples = target_samples
        else:
            print(f"  Randomly sampling {num_samples} from {len(target_samples)} samples")
            selected_samples = random.sample(target_samples, num_samples)
    
    else:
        # æ–¹æ³•3: éšæœºé‡‡æ ·ï¼ˆå¤‡ç”¨ï¼‰
        print("âš  No difficulty information available, using random sampling")
        all_samples = list(dataset)
        selected_samples = random.sample(all_samples, min(num_samples, len(all_samples)))
    
    return selected_samples


def save_to_database(samples: List[dict], dataset_name: str, overwrite: bool = False):
    """ä¿å­˜æ ·æœ¬åˆ°æ•°æ®åº“"""
    print("\n" + "="*70)
    print("ğŸ’¾ Saving to Database")
    print("="*70)
    
    with SQLModelUtils.create_session() as session:
        try:
            # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å·²å­˜åœ¨
            existing = session.exec(
                select(DatasetSample).where(DatasetSample.dataset == dataset_name)
            ).first()
            
            if existing:
                if not overwrite:
                    print(f"\nâš  Dataset '{dataset_name}' already exists in database!")
                    response = input("Do you want to overwrite it? (yes/no): ")
                    if response.lower() != 'yes':
                        print("âŒ Aborted")
                        return False
                
                # åˆ é™¤ç°æœ‰æ•°æ®
                existing_all = session.exec(
                    select(DatasetSample).where(DatasetSample.dataset == dataset_name)
                ).all()
                for item in existing_all:
                    session.delete(item)
                session.commit()
                print(f"âœ“ Deleted {len(existing_all)} existing samples")
            
            # æ·»åŠ æ–°æ ·æœ¬
            new_samples = []
            for idx, sample in enumerate(samples):
                # å°† puzzle ä½œä¸º questionï¼Œsolution ä½œä¸º answer
                question = sample.get('puzzle', sample.get('question', ''))
                answer = sample.get('solution', sample.get('answer', ''))
                
                # ç¡®ä¿æ˜¯å­—ç¬¦ä¸²ç±»å‹
                if isinstance(question, dict):
                    question = json.dumps(question, ensure_ascii=False)
                else:
                    question = str(question) if question else ""
                
                if isinstance(answer, dict):
                    answer = json.dumps(answer, ensure_ascii=False)
                else:
                    answer = str(answer) if answer else ""
                
                db_sample = DatasetSample(
                    dataset=dataset_name,
                    index=idx + 1,  # ç´¢å¼•ä»1å¼€å§‹
                    source="training_free_grpo",
                    question=question,
                    answer=answer,
                )
                new_samples.append(db_sample)
            
            session.add_all(new_samples)
            session.commit()
            print(f"âœ… Successfully saved {len(new_samples)} samples to '{dataset_name}'")
            return True
        
        except Exception as e:
            session.rollback()
            print(f"âŒ Error saving to database: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    parser = argparse.ArgumentParser(
        description="ä»ZebraLogicåŸå§‹æ•°æ®é›†ï¼ˆ1000é¢˜ï¼‰ä¸­é€‰æ‹©ä¸­ä¸‹ç­‰éš¾åº¦çš„100é“é¢˜ç›®"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=100,
        help="è¦é€‰æ‹©çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ï¼š100ï¼‰"
    )
    parser.add_argument(
        "--dataset_name",
        type=str,
        default="ZebraLogic-MediumLower-100",
        help="ä¿å­˜çš„æ•°æ®é›†åç§°"
    )
    parser.add_argument(
        "--analyze_only",
        action="store_true",
        help="åªåˆ†ææ•°æ®é›†ï¼Œä¸ä¿å­˜"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=42,
        help="éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰"
    )
    parser.add_argument(
        "--overwrite",
        action="store_true",
        help="å¦‚æœæ•°æ®é›†å·²å­˜åœ¨ï¼Œè‡ªåŠ¨è¦†ç›–ï¼ˆä¸è¯¢é—®ï¼‰"
    )
    
    args = parser.parse_args()
    
    print("ğŸš€ ZebraLogic Medium-Lower Difficulty Dataset Preparation")
    print("="*70)
    print(f"Target: {args.num_samples} samples of medium-lower difficulty")
    print("="*70)
    
    # 1. åŠ è½½åŸå§‹æ•°æ®é›†
    dataset = load_zebralogic_dataset()
    
    # 2. åˆ†æéš¾åº¦åˆ†å¸ƒ
    difficulty_field, difficulty_stats = analyze_difficulty(dataset)
    
    if args.analyze_only:
        print("\nâœ“ Analysis complete (--analyze_only mode)")
        print("\nTo create the dataset, run:")
        print(f"  uv run python scripts/data/prepare_zebralogic_medium_lower_100.py --num_samples {args.num_samples}")
        return
    
    # 3. é€‰æ‹©ä¸­ä¸‹ç­‰éš¾åº¦æ ·æœ¬
    selected_samples = select_medium_lower_samples(
        dataset,
        difficulty_field=difficulty_field,
        difficulty_stats=difficulty_stats,
        num_samples=args.num_samples,
        seed=args.seed,
    )
    
    print(f"\nâœ“ Selected {len(selected_samples)} samples")
    
    # 4. ä¿å­˜åˆ°æ•°æ®åº“
    success = save_to_database(
        selected_samples,
        args.dataset_name,
        overwrite=args.overwrite
    )
    
    if success:
        print("\n" + "="*70)
        print("âœ… All Done!")
        print("="*70)
        print(f"\nğŸ“Š Dataset created:")
        print(f"   Name: {args.dataset_name}")
        print(f"   Samples: {len(selected_samples)}")
        print(f"   Difficulty: Medium-Lower")
        print(f"\nğŸ“ Next steps:")
        print(f"   1. Update your training config to use '{args.dataset_name}'")
        print(f"   2. Example:")
        print(f"      data:")
        print(f"        practice_dataset_name: \"{args.dataset_name}\"")
        print(f"        batch_size: {min(30, len(selected_samples))}")
        print(f"   3. Run training:")
        print(f"      uv run python scripts/run_training_free_GRPO.py --config_name your_config")


if __name__ == "__main__":
    main()

