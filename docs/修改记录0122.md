## 2026-01-22 修改记录（经验积累/训练增强）

本次修改聚焦“经验积累在不同环境下更通用、更稳定”，并为后续课程学习/自博弈准备基础设施（错题集、反事实对比、检索式注入接口）。

### 为什么改

1. **经验筛选过窄**：原先经验生成只处理 reward 均值在 `(0,1)` 的“部分正确”组，导致 0/1 任务、reward>1 任务、全对/全错组基本无法积累经验。
2. **经验信号太稀疏**：缺少“同题多次尝试”的反事实对比（best vs worst），经验容易泛泛而谈。
3. **经验池会膨胀/重复**：L0/L1/L2 以及训练过程缺少谨慎去重与“高价值失败优先”的采样机制。
4. **全量注入提示词不可扩展**：经验越来越多时，全部拼到 instructions 会造成 prompt 变长、噪声变大。

### 怎么改（代码改动点）

#### 1) 经验筛选逻辑改为“全对全错也学习”

- 文件：`utu/practice/experience_updater.py`
- 变化：
  - 不再假设 reward 在 `(0,1)`，并且不再只挑“部分正确”组。
  - 每个问题组会选取少量代表性尝试用于总结（默认 best/worst + 少量补位），控制成本同时支持反事实对比。

#### 2) 引入反事实对比（counterfactual）

- 文件：`utu/practice/experience_updater.py`
- 变化：
  - 对同一问题的多次尝试，按 reward 选出 BEST/WORST（若无差异则标注 ONLY/无对比）。
  - 在 group advantage 阶段输出明确的对比结构，促使模型写出“为什么这个动作更好”的经验。

#### 3) 轨迹提取更通用（适配不同 trajectories 结构）

- 文件：`utu/practice/experience_updater.py`
- 变化：
  - 增强 `trajectories` 的解析：支持
    - `[{ "trajectory": [...] }]`（agent/tool 轨迹）
    - `[{...}, {...}]`（环境状态列表，如游戏轨迹）
  - `trajectories` 为空时也会进入总结流程（仅认 `trajectories` 字段，不兼容旧的 `trajectory` 字段）。
  - 统一用 JSON pretty-print + 截断的方式提供给 LLM，总结模板更通用。

#### 4) L0 谨慎去重（只在非常相似时跳过）

- 文件：`utu/practice/hierarchical_experience_manager.py`
- 变化：
  - 在写入 L0 前，对最近窗口（默认 50 条）做高阈值相似度检查（Jaccard，阈值默认 0.95）。
  - 改为同 scope 内（per-problem/per-game 启发式提取）才去重；没有 scope 就不去重，避免跨任务误杀。
  - 原则：**除非非常相似，否则不要去重**，避免误杀有效经验变体。

#### 5) 训练错题集（Mistake Bank）+ 下一轮优先抽“近期失败/高价值失败”

- 新增：`utu/practice/mistake_bank.py`
- 接入点：
  - `utu/practice/rollout_manager.py`：每个 batch judged 后更新错题集
  - `utu/practice/data_manager.py`：下一轮 `load_epoch_data()` 时按错题集进行偏置采样
- 行为：
  - 每条题（`dataset + dataset_index`）维护 1 条“活跃失败经验”：
    - 若失败：更新/覆盖失败经验（非常相似则不更新）
    - 一旦做对：记录成功经验，并删除“活跃失败经验”
    - 判定规则：`reward < 0.5` 视为失败，`reward >= 0.5` 视为成功；仅在 reward 缺失时回退使用 `correct`
  - 同一游戏（`meta.game_name`）内做谨慎去重：若新失败经验与该游戏中已有失败经验“非常相似”，会复用已有经验文本，避免在不同 seed 上重复堆叠几乎同样的 L0。
  - 训练采样不再均匀：优先抽取“近期失败/高价值失败”（新近失败、reward 低、重复失败次数多）。

#### 6) 检索式注入接口（暂不接入主流程）

- 新增：`utu/practice/experience_retriever.py`
- 说明：
  - 提供轻量级文本检索（bag-of-words + IDF）与注入渲染接口
  - **目前只提供代码与接口，不会改变现有训练/评估流程**

#### 7) 提示词模板更新（更通用 reward 描述 + 增加 reward/response 字段）

- 文件：`utu/prompts/practice/experience.yaml`
- 变化：
  - 不再写死 “reward 0~1”
  - 在单条轨迹总结里增加 `<Reward>`、`<Working Agent Response>` 字段

### 怎么用（最小用法）

#### 错题集文件位置

- 默认路径：`workspace/mistake_bank/<exp_id>.json`
- 在训练时自动生成/更新（无需额外命令）。

#### 控制“错题优先抽样”的比例

- 环境变量：`UTU_MISTAKE_FOCUS_RATIO`
  - 取值范围：`0.0 ~ 1.0`
  - 含义：每个 epoch 中优先抽样来自错题集（failed）的样本占比
  - 默认：`0.3`（只有在错题集存在后才会生效）

示例：

```bash
export UTU_MISTAKE_FOCUS_RATIO=0.5
```

#### 检索式注入（仅接口示例，不接入流程）

`utu/practice/experience_retriever.py` 可用于：
- 将经验池建立索引
- 对当前 query/state 检索 top-k 经验
- 渲染成可注入的 prompt block

（后续如果要接入，可在 agent.run 前对当前输入检索并拼接。）
